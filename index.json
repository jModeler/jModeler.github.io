[{"authors":["admin"],"categories":null,"content":"I currently work as a Data Scientist and am interested in bringing together ideas from Economics, Statistics, Computer Science (and sometimes Physics) to help answer pertinent business questions.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I currently work as a Data Scientist and am interested in bringing together ideas from Economics, Statistics, Computer Science (and sometimes Physics) to help answer pertinent business questions.","tags":null,"title":"Journeyman Modeler","type":"authors"},{"authors":[],"categories":["Bayesian Nonparametrics"],"content":"\rThis post is a continuation of the post that constructs a Dirichlet distribution from Gamma distributed variables. We use the same parameterization of the Gamma Distribution as before, and set the rate parameter \\(=1\\) unless otherwise specified. In this post I prove that the following property holds:\nIf \\(U = (U_1,\\dots,U_{i},U_{i+1}, \\dots, U_{N}) \\sim Dir(\\alpha_1,\\dots,\\alpha_i,\\alpha_{i+1},\\dots, \\alpha_N)\\) then \\(U\u0026#39; = (U_1,U_2,\\dots,U_{i}+U_{i+1}, \\dots, U_{N}) \\sim Dir(\\alpha_1,\\alpha_2,\\dots,\\alpha_i+\\alpha_{i+1},\\dots, \\alpha_N)\\)\nIn general, \\(U\u0026#39;\u0026#39; = (\\sum\\limits_{i=1}^{k_1} U_i, \\sum\\limits_{i=k_1+1}^{k_2} U_i, \\dots, \\sum\\limits_{i=k_j+1}^{N} U_i ) \\sim Dir(\\sum\\limits_{i=1}^{k_1} \\alpha_i, \\sum\\limits_{i=k_1+1}^{k_2} \\alpha_i, \\dots, \\sum\\limits_{i=k_j+1}^{N} \\alpha_i )\\)\nI prove the first part of this property here. The rest follows by simply extending the proof. The strategy here is to construct IID Gamma distributed variables with appropriate parameters, and then derive the distribution of \\(U\u0026#39;\\) using the results from the previous post.\nAssume \\(Z_i\\)’s are IID \\(\\sim Gamma(\\alpha_i,1)\\). From the additive property of the Gamma distribution, I have \\(Z_{i,i+1} = Z_i + Z_{i+1} \\sim Gamma(\\alpha_i+\\alpha_{i+1},1)\\). By definition, \\(Z_{i,i+1}\\) is independent of \\(Z_j\\)’s for \\(1 \\le j \\le N, j \\neq i, i+1\\).\nI now define \\(U\u0026#39; = (U_1,U_2,\\dots,U_{i}+U_{i+1}, \\dots, U_{N}) = (U_1,U_2,\\dots,U_{i,i+1}, \\dots, U_{N})\\), where \\(U_j = \\frac{Z_j}{\\sum\\limits_{j\\neq i, i+1}^{N} Z_j + Z_{i,i+1}}\\). From here, it follows that \\(U\u0026#39; \\sim Dir(\\alpha_1,\\alpha_2,\\dots,\\alpha_i+\\alpha_{i+1},\\dots, \\alpha_N)\\). The second part of theorem follows from similar reasoning.\nThe result above can be used to get the marginal distributions of the \\(U_i\\)’s, where \\(U = (U_1,\\dots,U_i,\\dots,U_N)\\).\nDefine \\(U\u0026#39; = (U_i, \\sum\\limits_{j \\neq i}^{N} U_j)\\).\nFrom the result above, this is distributed \\(Dir(\\alpha_i, \\sum\\limits_{j \\neq i}^{N} \\alpha_j) = Beta(\\alpha_i, \\sum\\limits_{j \\neq i}^{N} \\alpha_j)\\).\nSince \\(\\sum\\limits_{j \\neq i}^{N} U_j = 1 - U_i\\), the density function can be expressed purely as a function of \\(U_i\\), which is the marginal distribution of \\(U_i\\).\nFormally:\nIf \\(U = (U_1, \\dots, U_{N}) \\sim Dir(\\alpha_1,\\dots, \\alpha_N)\\) then \\(U_i \\sim Beta(\\alpha_i, \\sum\\limits_{j \\neq i}^{N} \\alpha_j)\\)\nThe Complete Neutral Property of the Dirichlet Distribution\rA somewhat related property is also discussed here:\nIf \\(U = (U_1, \\dots, U_{N}) \\sim Dir(\\alpha_1,\\dots, \\alpha_N)\\) and when \\(k \u0026lt; N\\), then\n\\(\\frac{1}{(1-\\sum\\limits_{j = k+1}^{N} U_j)}(U_1,\\dots,U_k) \\sim Dir(\\alpha_1,\\dots,\\alpha_k)\\)\nHere, we note the term that divides each component of the vector \\((U_1,\\dots,U_k)\\) is \\((1-\\sum\\limits_{j = k+1}^{N} U_j)\\), which is nothing but \\(\\sum\\limits_{j = 1}^{k} U_j\\), since \\(\\sum\\limits_{j = 1}^{N} U_j = 1\\).\nAssume \\(\\{Z_i\\}_{i=1}^{N}\\), are \\(N\\) IID Gamma distributed random variables, with \\(Z_i \\sim Gamma(\\alpha_i,1) \\, \\forall \\, i=1,\\dots,N\\).\nIn the post on constructing a dirichlet distribution from Gamma distributed variables, it was shown that when \\(U_i = Z_i/(\\sum\\limits_{j = 1}^{N} Z_j) \\, \\forall \\, i=1,\\dots,N\\), then \\(U = (U_1, \\dots, U_{N}) \\sim Dir(\\alpha_1,\\dots, \\alpha_N)\\). With this construction, I now have:\n\\[\\begin{align}\rX_i = \\frac{U_i}{1-\\sum\\limits_{j = k+1}^{N} U_j} = \\frac{U_i}{\\sum\\limits_{j = 1}^{k} U_j} \\, \\forall \\, i=1,\\dots,k \\tag{1} \\end{align}\\]\nSince \\(U_i = Z_i/(\\sum\\limits_{j = 1}^{N} Z_j) \\, \\forall \\, i=1,\\dots,N\\), I put this in (1) to get:\n\\[\\begin{align}\rX_i = \\frac{U_i}{\\sum\\limits_{j = 1}^{k} U_j} = \\frac{Z_i}{\\sum\\limits_{j = 1}^{k} Z_j} \\, \\forall \\, i=1,\\dots,k \\tag{2}\r\\end{align}\\]\nFrom the post on constructing a dirichlet distribution from Gamma distributed variables, we know that \\(X = (X_1,\\dots, X_k) \\sim Dir(\\alpha_1,\\dots,\\alpha_k)\\), which completes the proof. This property of the Dirichlet Distribution is also called the \\(\\textit{complete neutral property}\\) (Albert and Denis 2011).\n\rReferences\rAlbert, Isabelle, and Jean-Baptiste Denis. 2011. “Dirichlet and multinomial distributions: properties and uses in JAGS.” Analysis 31: 1141–55.\n\r\r\r","date":1591488000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591533586,"objectID":"20d59d2f5c5af0eeb8a151e5c9038269","permalink":"/post/a-useful-property-of-the-dirichlet-distribution/","publishdate":"2020-06-07T00:00:00Z","relpermalink":"/post/a-useful-property-of-the-dirichlet-distribution/","section":"post","summary":"This post is a continuation of the post that constructs a Dirichlet distribution from Gamma distributed variables. We use the same parameterization of the Gamma Distribution as before, and set the rate parameter \\(=1\\) unless otherwise specified. In this post I prove that the following property holds:\nIf \\(U = (U_1,\\dots,U_{i},U_{i+1}, \\dots, U_{N}) \\sim Dir(\\alpha_1,\\dots,\\alpha_i,\\alpha_{i+1},\\dots, \\alpha_N)\\) then \\(U\u0026#39; = (U_1,U_2,\\dots,U_{i}+U_{i+1}, \\dots, U_{N}) \\sim Dir(\\alpha_1,\\alpha_2,\\dots,\\alpha_i+\\alpha_{i+1},\\dots, \\alpha_N)\\)\nIn general, \\(U\u0026#39;\u0026#39; = (\\sum\\limits_{i=1}^{k_1} U_i, \\sum\\limits_{i=k_1+1}^{k_2} U_i, \\dots, \\sum\\limits_{i=k_j+1}^{N} U_i ) \\sim Dir(\\sum\\limits_{i=1}^{k_1} \\alpha_i, \\sum\\limits_{i=k_1+1}^{k_2} \\alpha_i, \\dots, \\sum\\limits_{i=k_j+1}^{N} \\alpha_i )\\)","tags":["Dirichlet Distribution","Beta Distribution","Gamma Distribution"],"title":"A Useful Property of the Dirichlet Distribution","type":"post"},{"authors":[],"categories":["Bayesian Nonparametrics"],"content":"\rThis post is a continuation of the post that constructs a Beta distribution from Gamma distributed variables. We use the same parameterization of the Gamma Distribution as before, and set the rate parameter \\(=1\\) unless otherwise specified. In this post I prove that the following property holds:\nGiven \\(Z_i \\sim Gamma(\\alpha_i,1)\\), are IID, (\\(\\alpha_i \u0026gt; 0\\) and \\(1 \\le i \\le n\\)) and I define the variables \\(U_i = \\frac{Z_i}{\\sum\\limits_{i=1}^{N} Z_i}\\) and \\(V = \\sum\\limits_{i=1}^{N} Z_i\\). Then:\n\r\\((U_1, U_2, \\dots, U_N) \\sim Dir(\\alpha_1, \\alpha_2, \\dots, \\alpha_N) = \\frac{\\Gamma(\\sum \\alpha_i)}{\\Gamma(\\alpha_1)\\dots \\Gamma(\\alpha_N)} u_1^{\\alpha_1-1}u_2^{\\alpha_1-1}\\dots u_N^{\\alpha_N-1} = \\frac{\\Gamma(\\sum \\alpha_i)}{\\Gamma(\\alpha_1)\\dots \\Gamma(\\alpha_N)} u_1^{\\alpha_1-1}u_2^{\\alpha_1-1}\\dots (1-\\sum\\limits_{i=1}^{N-1}u_i)^{\\alpha_N-1}\\)\r\\(V \\sim Gamma(\\sum\\limits_{i=1}^{N} \\alpha_i, 1)\\)\r\\(U = (U_1, U_2, \\dots, U_N)\\) and \\(V\\) are independent\r\rBy construction, \\(\\sum\\limits_{i=1}^{N} U_i = 1\\). The \\(N^{th}\\) term is redundant (in the sense that it can be obtained from the remaining \\((N-1)\\) terms). I use the change of variables approach to find the joint distribution of \\((U_1,U_2,\\dots,U_{N-1}, V)\\). I represent the \\(Z_i\\)’s as functions of \\(U_i\\) and \\(V\\) to get:\n\\[\\begin{align}\rZ_i \u0026amp; = U_i V \u0026amp;\u0026amp; 0 \\le i \\le N-1 \\tag{1}\\\\ Z_N \u0026amp; = (1-\\sum_{i=1}^{N-1}U_i)V \u0026amp;\u0026amp; \\tag{2}\r\\end{align}\\]\nBy definition, the joint density of \\((U_1,U_2,\\dots,U_{N-1}, V)\\) is given by:\r\\[\\begin{align}\rf_{U_1,U_2,\\dots,U_{N-1}, V}(u_1,u_2,\\dots, u_{N-1}, v) \u0026amp;= f_{Z_1,\\dots,Z_N}(z_1,\\dots,z_N) |J_{(z_1,z_2,\\dots,z_{N-1},z_N) \\rightarrow (u_1,u_2,\\dots, u_{N-1}, v)}| \u0026amp; \\tag{3}\r\\end{align}\\]\nThe Jacobian matrix is given by:\r\\[\\begin{align}\rJ_{(z_1,z_2,\\dots,z_{N-1},z_N) \\rightarrow (u_1,u_2,\\dots, u_{N-1}, v)} = \\begin{bmatrix} \\frac{\\partial z_1}{\\partial u_1} \u0026amp; \\frac{\\partial z_1}{\\partial u_2} \u0026amp; \\dots \u0026amp; \\frac{\\partial z_1}{\\partial u_{N-1}} \u0026amp; \\frac{\\partial z_1}{\\partial v} \\\\ \\frac{\\partial z_2}{\\partial u_1} \u0026amp; \\frac{\\partial z_2}{\\partial u_2} \u0026amp; \\dots \u0026amp; \\frac{\\partial z_2}{\\partial u_{N-1}} \u0026amp; \\frac{\\partial z_2}{\\partial v} \\\\\r\\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \u0026amp; \\vdots \\\\\r\\frac{\\partial z_{N-1}}{\\partial u_1} \u0026amp; \\frac{\\partial z_{N-1}}{\\partial u_2} \u0026amp; \\dots \u0026amp; \\frac{\\partial z_{N-1}}{\\partial u_{N-1}} \u0026amp; \\frac{\\partial z_{N-1}}{\\partial v} \\\\\r\\frac{\\partial z_N}{\\partial u_1} \u0026amp; \\frac{\\partial z_N}{\\partial u_2} \u0026amp; \\dots \u0026amp; \\frac{\\partial z_N}{\\partial u_N} \u0026amp; \\frac{\\partial z_N}{\\partial v} \\end{bmatrix} \\tag{4}\r\\end{align}\\]\nGiven (1) \u0026amp; (2), the partial derivatives in the Jacobian matrix are given by\n\\[\\begin{alignat}{2}\r\\frac{\\partial z_i}{\\partial u_i} \u0026amp; = v \u0026amp;\u0026amp; \\quad 0 \\le i \\le N-1 \\tag{5}\\\\\r\\frac{\\partial z_i}{\\partial u_j} \u0026amp; = 0 \u0026amp;\u0026amp; \\quad j \\neq i, 0 \\le j \\le N-1 \\tag{6}\\\\\r\\frac{\\partial z_i}{\\partial v} \u0026amp; = u_i \u0026amp;\u0026amp; \\quad 0 \\le i \\le N-1 \\tag{7} \\\\\r\\frac{\\partial z_N}{\\partial u_i} \u0026amp; = -v \u0026amp;\u0026amp; \\quad 0 \\le i \\le N-1 \\tag{8} \\\\\r\\frac{\\partial z_N}{\\partial v} \u0026amp; = 1-\\sum_{i=1}^{N-1} u_i \\tag{9}\r\\end{alignat}\\]\nSubstituting (5), (6), (7), (8) \u0026amp; (9) in (4), we get\r\\[\\begin{align}\rJ_{(z_1,z_2,\\dots,z_{N-1},z_N) \\rightarrow (u_1,u_2,\\dots, u_{N-1}, v)} = \\begin{bmatrix} v \u0026amp; 0 \u0026amp; \\dots \u0026amp; 0 \u0026amp; u_1 \\\\ 0 \u0026amp; v \u0026amp; \\dots \u0026amp; 0 \u0026amp; u_2 \\\\\r\\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \u0026amp; \\vdots \\\\\r0 \u0026amp; 0 \u0026amp; \\dots \u0026amp; v \u0026amp; u_{N-1} \\\\\r-v \u0026amp; -v \u0026amp; \\dots \u0026amp; -v \u0026amp; 1-\\sum_{i=1}^{N-1} u_i \\end{bmatrix} \\tag{10}\r\\end{align}\\]\nI need the determinant of the Jacobian matrix above. This can be computed easily after making a row transformation operation. Apply the transformation \\(R_N \\rightarrow R_N + \\sum\\limits_{i=1}^{N-1} R_i\\) (where \\(R_i\\) is the \\(i^{th}\\) row of the determinant) to get:\n\\[\\begin{align}\r|J_{(z_1,z_2,\\dots,z_{N-1},z_N) \\rightarrow (u_1,u_2,\\dots, u_{N-1}, v)}| = \\begin{vmatrix} v \u0026amp; 0 \u0026amp; \\dots \u0026amp; 0 \u0026amp; u_1 \\\\ 0 \u0026amp; v \u0026amp; \\dots \u0026amp; 0 \u0026amp; u_2 \\\\\r\\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \u0026amp; \\vdots \\\\\r0 \u0026amp; 0 \u0026amp; \\dots \u0026amp; v \u0026amp; u_{N-1} \\\\\r0 \u0026amp; 0 \u0026amp; \\dots \u0026amp; 0 \u0026amp; 1 \\end{vmatrix} = v^{N-1} \\tag{11}\r\\end{align}\\]\nUsing result (11) in (3), I get:\n\\[\\begin{align}\rf_{U_1,U_2,\\dots,U_{N-1}, V}(u_1,u_2,\\dots, u_{N-1}, v) = f_{Z_1,Z_2,\\dots,Z_{N-1},Z_N}(z_1,z_2,\\dots,z_{N-1},z_N) v^{N-1} \\tag{12}\r\\end{align}\\]\nSince \\(Z_i\\) are IID \\(Gamma(\\alpha_i,1)\\) distributed:\r\\[\\begin{align}\rf_{Z_1,\\dots,Z_N}(z_1,\\dots,z_N) = \\prod_{i=1}^{N}f_{Z_i}(z_i) = \\prod_{i=1}^{N} \\frac{1}{\\Gamma(\\alpha_i)} z_i^{\\alpha_i - 1} e^{-z_i} = \\left[ \\prod_{i=1}^{N} \\frac{1}{\\Gamma(\\alpha_i)} z_i^{\\alpha_i - 1} \\right] e^{-\\sum\\limits_{i=1}^{N}z_i} \\tag{13}\r\\end{align}\\]\nSubstituting (13) into (12), and using the relations in (1) \u0026amp; (2):\r\\[\\begin{align}\rf_{U_1,\\dots,U_{N-1}, V}(u_1,\\dots, u_{N-1}, v) = \u0026amp; \\left[ \\prod_{i=1}^{N} \\frac{1}{\\Gamma(\\alpha_i)} z_i^{\\alpha_i - 1} \\right] e^{-\\sum\\limits_{i=1}^{N}z_i} \\cdot v^{N-1} \u0026amp; \\nonumber \\\\\r\\implies f_{U_1,\\dots,U_{N-1}, V}(u_1,\\dots, u_{N-1}, v) = \u0026amp; \\left[ \\prod_{i=1}^{N-1} \\frac{1}{\\Gamma(\\alpha_i)} z_i^{\\alpha_i - 1} \\right] \\frac{1}{\\Gamma(\\alpha_N)} z_N^{\\alpha_N - 1} e^{-v} v^{N-1} \u0026amp; \\nonumber \\\\\r\\implies f_{U_1,\\dots,U_{N-1}, V}(u_1,\\dots, u_{N-1}, v) = \u0026amp; \\left[ \\prod_{i=1}^{N-1} \\frac{1}{\\Gamma(\\alpha_i)} (u_i v)^{\\alpha_i - 1} \\right] \\frac{1}{\\Gamma(\\alpha_N)} (v(1-\\sum\\limits_{i=1}^{N-1}u_i))^{\\alpha_N - 1} e^{-v} v^{N-1} \u0026amp; \\tag{14}\r\\end{align}\\]\nSince \\(U_N = 1 - \\sum\\limits_{i=1}^{N-1}U_i\\), I put this in (14) to get\r\\[\\begin{align}\rf_{U_1,\\dots,U_{N-1}, V}(u_1,\\dots, u_{N-1}, v) \u0026amp; = \\left[ \\prod_{i=1}^{N-1} \\frac{1}{\\Gamma(\\alpha_i)} (u_i v)^{\\alpha_i - 1} \\right] \\frac{1}{\\Gamma(\\alpha_N)} (u_N v)^{\\alpha_N - 1} e^{-v} v^{N-1} \u0026amp; \\nonumber \\\\\r\\implies f_{U_1,\\dots,U_{N-1}, V}(u_1,\\dots, u_{N-1}, v) \u0026amp; = \\left[ \\prod_{i=1}^{N} \\frac{1}{\\Gamma(\\alpha_i)} (u_i v)^{\\alpha_i - 1} \\right] e^{-v} v^{N-1} \u0026amp; \\nonumber \\\\\r\\implies f_{U_1,\\dots,U_{N-1}, V}(u_1,\\dots, u_{N-1}, v) \u0026amp; = \\left[ \\prod_{i=1}^{N} \\frac{1}{\\Gamma(\\alpha_i)} (u_i)^{\\alpha_i - 1} \\right] v^{\\left(\\sum\\limits_{i=1}^{N}\\alpha_i-N \\right)} e^{-v} v^{N-1} \u0026amp; \\nonumber \\\\\r\\implies f_{U_1,\\dots,U_{N-1}, V}(u_1,\\dots, u_{N-1}, v) \u0026amp; = \\left[ \\prod_{i=1}^{N} \\frac{1}{\\Gamma(\\alpha_i)} (u_i)^{\\alpha_i - 1} \\right] v^{\\left(\\sum\\limits_{i=1}^{N}\\alpha_i-1 \\right)} e^{-v} \u0026amp; \\nonumber \\end{align}\\]\n\\[\\begin{align}\r\\implies f_{U_1,\\dots,U_{N-1}, V}(u_1,\\dots, u_{N-1}, v) \u0026amp; = \\Gamma \\left(\\sum\\limits_{i=1}^{N}\\alpha_i \\right) \\left[ \\prod_{i=1}^{N} \\frac{1}{\\Gamma(\\alpha_i)} (u_i)^{\\alpha_i - 1} \\right] \\frac{1}{\\Gamma \\left(\\sum\\limits_{i=1}^{N}\\alpha_i \\right)}v^{\\left(\\sum\\limits_{i=1}^{N}\\alpha_i-1 \\right)} e^{-v} \u0026amp; \\nonumber \\\\\r\\implies f_{U_1,\\dots,U_{N-1}, V}(u_1,\\dots, u_{N-1}, v) \u0026amp; = \\frac{\\Gamma \\left(\\sum\\limits_{i=1}^{N}\\alpha_i \\right)}{\\prod\\limits_{i=1}^{N} \\Gamma(\\alpha_i)} \\left[ \\prod_{i=1}^{N} (u_i)^{\\alpha_i - 1} \\right] \\frac{1}{\\Gamma \\left(\\sum\\limits_{i=1}^{N}\\alpha_i \\right)}v^{\\left(\\sum\\limits_{i=1}^{N}\\alpha_i-1 \\right)} e^{-v} \u0026amp; \\nonumber \\\\\r\\implies f_{U_1,\\dots,U_{N-1}, V}(u_1,\\dots, u_{N-1}, v) \u0026amp; = Dir(\\alpha_1,\\alpha_2, \\dots, \\alpha_N) \\cdot Gamma(\\sum\\limits_{i=1}^{N}\\alpha_i, 1) \u0026amp; \\end{align}\\]\rWhich establishes the following:\n\\((U_1,U_2,\\dots,U_{N-1}, U_{N}) \\sim Dir(\\alpha_1,\\alpha_2, \\dots, \\alpha_N)\\)\r\\(V \\sim Gamma(\\sum\\limits_{i=1}^{N}\\alpha_i,1)\\)\r\\(U = (U_1,U_2,\\dots,U_{N-1}, U_{N})\\) \u0026amp; \\(V\\) are independent\r\rThis completes the proof.\n","date":1590883200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590925462,"objectID":"3cff9dc0d4ae459e32a62f46657b93ec","permalink":"/post/deriving-the-dirichlet-distribution-from-gamma-distributed-variables/","publishdate":"2020-05-31T00:00:00Z","relpermalink":"/post/deriving-the-dirichlet-distribution-from-gamma-distributed-variables/","section":"post","summary":"This post is a continuation of the post that constructs a Beta distribution from Gamma distributed variables. We use the same parameterization of the Gamma Distribution as before, and set the rate parameter \\(=1\\) unless otherwise specified. In this post I prove that the following property holds:\nGiven \\(Z_i \\sim Gamma(\\alpha_i,1)\\), are IID, (\\(\\alpha_i \u0026gt; 0\\) and \\(1 \\le i \\le n\\)) and I define the variables \\(U_i = \\frac{Z_i}{\\sum\\limits_{i=1}^{N} Z_i}\\) and \\(V = \\sum\\limits_{i=1}^{N} Z_i\\).","tags":["Gamma Distribution","Dirichlet Distribution"],"title":"Deriving the Dirichlet Distribution from Gamma Distributed Variables","type":"post"},{"authors":[],"categories":["Bayesian Nonparametrics"],"content":"\rThis post is a continuation of the post that constructs a Dirichlet distribution from Gamma distributed variables. I now provide an empirical example of this construction for a simple case. In all of the work that follows, Unless otherwise specified, the rate parameter for the Gamma distribution is 1.\nI follow the steps below to do this:\nGenerate data from a gamma distribution\rApply the appropriate transformation to these\rCheck the distribution of the transformed variables\rCompare against data generated from a Dirichlet distribution with appropriate parameters\r\rPreliminaries\rI write a function that generates IID draws from \\(d\\) different gamma distributions, all with rate parameter = 1. Note that this is purely a utility function with no sanity checks on the input.\rIn the code below, the parameter \\(d\\) is implicit, in that it is the length of the vector of shape parameters, \\(alpha\\), which is being sent as an input (all the elements in the vector \\(alpha\\) must be \\(\u0026gt;0\\)).\n#Function to generate n random draws of dimension from d different gamma distributions\rmv_gamma \u0026lt;- function(rgamma, n, alpha)\r{\r# rgamma: the base r function to generate random draws from a univariate gamma distribution\r# n: number of draws needed\r# alpha: vector of shape parameters\r# generate random draws for each dimension, then put them together into an array\r# using sapply for this\rmvg_draws \u0026lt;- sapply(alpha, function(x) rgamma(n, x))\r#return this value\rreturn(mvg_draws)\r}\rI create another utility function to transform the output from the previous function to construct Dirichlet distributed variables.\n#Function to construct dirichlet distributed variables from gamma distributed variables\rconstruct_dirichlet \u0026lt;- function(mvg_draws)\r{\r# mvg_draws: output from the mv_gamma function. Typically an n x d matrix. # Transform the columns of mvg_draws appropriately:\r# Divide each row with its sum\rconst_dirichlet \u0026lt;- mvg_draws/rowSums(mvg_draws)\r#return this value\rreturn(const_dirichlet)\r}\rNow, a utility function to compare the distrbution of the constructed and actual random variables. Note that I use the \\(\\texttt{cramer.test()}\\) function from the cramer package to compare the multivariate draws.\ncompare_distributions \u0026lt;- function(const_dirichlet, actual_dirichlet) {\r# const_dirichlet: n x d matrix containing the constructed dirichlet random draws\r# actual_dirichlet: n x d matrix containing the actual dirichlet random draws\r# get the cramer test result\rcompare \u0026lt;- cramer.test(const_dirichlet, actual_dirichlet)\r# check the result of this hypothesis test\rif(compare$result==0) {\rcat(\u0026#39;Unable to reject hypothesis of equal distributions\u0026#39;)\r} else {\rcat(\u0026#39;Hypothesis of equal distributions should not be accepted\u0026#39;)\r}\r}\rLastly, another utility function to compare (via plot) the distribution of the constructed and actual random variables. I use the \\(\\texttt{rdirichlet()}\\) function from the MCMCpack package to generate draws from the dirichlet distribution.\n#Function to plot densities of the constructed and actual dirichlet distribution\rplot_compare \u0026lt;- function(const_dirichlet, actual_dirichlet, d)\r{\r# const_dirichlet: n x d matrix containing the constructed dirichlet random draws\r# actual_dirichlet: n x d matrix containing the actual dirichlet random draws\r# d: the column id/component of the draw needed from both constucted and actual for comparison\r# Get the number of data points\rn \u0026lt;- nrow(const_dirichlet)\r# Get the draws in the dimension or column whose distribution need to be compared to the actual\rdcol \u0026lt;- const_dirichlet[,d]\r# Get the draws in the dimension or column, from the actual draws\rdcol_actual \u0026lt;- actual_dirichlet[,d]\r#create a data.frame of these values\rplot_data \u0026lt;- data.frame(draws = c(dcol, dcol_actual), type = rep(c(\u0026#39;constructed\u0026#39;, \u0026#39;actual\u0026#39;)), each = n)\r#plot this function\rggplot(data = plot_data, aes(x = draws, color = type)) + geom_density() + xlab(\u0026#39;draws\u0026#39;) + ylab(\u0026#39;density\u0026#39;)\r}\r\r3 Dimensional Case\rLet’s use the functions we just created. I follow the steps outlined in the introduction of this post.\nlibrary(cramer)\r## Loading required package: boot\rsuppressPackageStartupMessages(library(MCMCpack))\r# number of draws\rn \u0026lt;- 1000\r# multivariate shapre parameter\ralpha \u0026lt;- c(1,2,3)\r# get 3D IID gamma density draws\rset.seed(100)\rmvg_draws \u0026lt;- mv_gamma(rgamma, n, alpha)\r# get the dirichlet draws from construction\rconst_dirichlet \u0026lt;- construct_dirichlet(mvg_draws)\r# get the dirichlet draws from the actual distribution\rset.seed(100)\ractual_dirichlet \u0026lt;- rdirichlet(n, alpha)\r# check if the draws come from the same underlying distribution\rcompare_distributions(const_dirichlet, actual_dirichlet)\r## Unable to reject hypothesis of equal distributions\rAs can be seen above, the null hypothesis that the underlying distributions are the same cannot be rejected.\rLet’s also look at the empirical densities of each of the columns of the constructed and actual dirichlet distribution draws.\nDimension 1:\rDimension 2:\rDimension 3:\rThese density plots look somewhat similar. When we increase the number of draws generated from both methods, these plots almost overlap:\n# number of draws\rn \u0026lt;- 100000\r# multivariate shapre parameter\ralpha \u0026lt;- c(1,2,3)\r# get 3D IID gamma density draws\rset.seed(100)\rmvg_draws \u0026lt;- mv_gamma(rgamma, n, alpha)\r# get the dirichlet draws from construction\rconst_dirichlet \u0026lt;- construct_dirichlet(mvg_draws)\r# get the dirichlet draws from the actual distribution\rset.seed(100)\ractual_dirichlet \u0026lt;- rdirichlet(n, alpha)\rDimension 1:\rDimension 2:\rDimension 3:\rComparing the draws for different values of \\(d\\) (i.e. different lengths of the \\(alpha\\) vector), is left as an exercise to the reader. The functions above can be re-used for different values of \\(d\\), but must be modified accordingly to include sanity checks for inappropriate input values and edge cases, via unit testing.\n\r","date":1590883200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590930816,"objectID":"f67adfa223b18f83f3e0cd81759ab42e","permalink":"/post/dirichlet-distribution-from-gamma-distribution-simple-examples/","publishdate":"2020-05-31T00:00:00Z","relpermalink":"/post/dirichlet-distribution-from-gamma-distribution-simple-examples/","section":"post","summary":"This post is a continuation of the post that constructs a Dirichlet distribution from Gamma distributed variables. I now provide an empirical example of this construction for a simple case. In all of the work that follows, Unless otherwise specified, the rate parameter for the Gamma distribution is 1.\nI follow the steps below to do this:\nGenerate data from a gamma distribution\rApply the appropriate transformation to these\rCheck the distribution of the transformed variables\rCompare against data generated from a Dirichlet distribution with appropriate parameters\r\rPreliminaries\rI write a function that generates IID draws from \\(d\\) different gamma distributions, all with rate parameter = 1.","tags":["Dirichlet Distribution","Gamma Distribution"],"title":"Dirichlet Distribution from Gamma Distribution: Simple Examples","type":"post"},{"authors":[],"categories":["Scientific Computing"],"content":"\rThis post presents 2 simple examples of the homotopy principle applied to (fairly easy) linear and nonlinear systems of equations. At a very abstract level, given a system of equations for which a solution is needed, we convert this system to one whose solution we already know (or is easy to find out), and then bend this system till we get the solution to the original set of equations. These methods have very broad applications (Garcia and Zangwill 1981), and have been applied in the context of finding equlibria in Static Games (Bajari et al. 2010) and Dynamic Games (Borkovsky, Doraszelski, and Kryukov 2010).\nExample 1: Linear System\rSay we want to find the solution to the following system of linear equations:\r\\[\\begin{align}\r\\left[ \\begin{array}\r{rrrrr}\r1 \u0026amp; 2 \\\\\r3 \u0026amp; 4 \\\\\r\\end{array}\\right] \\left[ \\begin{array} {r} x_1 \\\\\rx_2 \\\\\r\\end{array} \\right] = \\left[ \\begin{array} {l} 5 \\\\\r11 \\\\\r\\end{array} \\right] \\tag{1} \\end{align}\\]\nReaders should very easily be able to verify that the unique solution to this system is\r\\[\r(x_1,x_2) = (1,2)\r\\]\nLet’s convert this system and introduce an additional parameter \\(t\\), called the homotopy parameter, which varies from \\(0\\) to \\(1\\). Let’s call this new system \\(H(x_1, x_2, t)\\)\r\\[\\begin{align}\r\\left[ \\begin{array}\r{rr}\r1 \u0026amp; 2 \\\\\r3 \u0026amp; 4 \\\\\r\\end{array}\\right] \\left[ \\begin{array} {r} x_1 \\\\\rx_2 \\\\\r\\end{array} \\right] = \\left[ \\begin{array} {l} 5t \\\\\r11t \\\\\r\\end{array} \\right] \\tag{2} \\end{align}\\]\nWhen \\(t=0\\), \\(H(x_1, x_2, 0)\\) yields the trivial (and only) solution \\((x_1,x_2) = (0,0)\\). When \\(t=1\\), we get our original system of equations back. When we solve for \\((x_1, x_2)\\) as a function of \\(t\\), we get:\r\\[\r(x_1(t),x_2(t)) = (t,2t)\r\\]\rAt \\(t=1\\), this will give us the solution we desire. Tracing the path of the solution gives us the following plots:\rFrom this very simple example, we note that the general process followed is given below (Garcia and Zangwill 1981):\nConvert the system of equations into one that has a known solution (i.e. \\(H(x_1, x_2, 0)\\) case above)\rIntroduce a new parameter \\(t\\), that gives the known system at \\(t=0\\) and the system for which the solutions are desired when \\(t=1\\)\rTrace the path of the solutions by changing the value of \\(t\\) from \\(0\\) to \\(1\\)\r\r\rExample 2: Nonlinear System\rConsider the following system of equations (from chapter 1, exercise 6 of (Garcia and Zangwill 1981)):\r\\[\\begin{align}\rF(x_1, x_2) = \\left[ \\begin{array}\r{l}\re^{2 x_1} - x^{2}_{2} + 3 \\\\\r4x_{2}e^{2 x_1} - x^{3}_{2} \\\\\r\\end{array} \\right]\r= \\left[ \\begin{array} {l} 0 \\\\\r0 \\\\\r\\end{array} \\right] \\tag{3} \\\\\r(x_1, x_2) \\in \\mathbb{R}^2 \\\\\rF: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2\r\\end{align}\\]\nAgain, readers should verify that the solutions to this system of equations are \\((x1, x2) = (0, -2) \\, \\\u0026amp; \\, (x1, x2) = (0, 2)\\).\nWe now introduce the homotopy parameter \\(t\\), and define \\(H(x_1, x_2, t)\\) as:\n\\[\\begin{align}\rH(x_1, x_2, t) = F(x_1, x_2) - (1-t)F(0, 0) \\tag{4}\r\\end{align}\\]\nThe formulation in (4) is called the Newton Homotopy (Garcia and Zangwill 1981). A distinct advantage of this formulation, is that at \\(t=0\\), it is easy to see that the solution to the system is \\((0, 0)\\).\nFor any \\(t\\) between \\(0\\) and \\(1\\), \\(H(x_1, x_2, t)\\) becomes:\r\\[\\begin{align}\r\\left[ \\begin{array}\r{l}\re^{2 x_1} - x^{2}_{2} + 4t - 1 \\\\\r4x_{2}e^{2 x_1} - x^{3}_{2} \\\\\r\\end{array} \\right]\r= \\left[ \\begin{array} {l} 0 \\\\\r0 \\\\\r\\end{array} \\right] \\tag{5}\r\\end{align}\\]\nWe now attempt to find \\((x_1, x_2)\\) as functions of \\(t\\). From (5), we have:\r\\[\\begin{align}\rx_{2}^{3} = 4x_{2}e^{2 x_1} \\nonumber \\\\\rx_2 = 0 \\,\\,\\,\\,\\, OR \\,\\,\\,\\,\\, x_{2} = \\pm 2e^{x_1} \\tag{6}\r\\end{align}\\]\nWhen \\(x_2 = 0\\), putting this back in (5) we get:\r\\[\\begin{align}\re^{2 x_1} + 4t - 1 = 0 \\nonumber \\\\\r\\implies x_1 = \\frac{1}{2} log(1-4t) \\tag{7} \\\\\rwhere \\,\\,\\,\\, 0 \\le t \\le 1/4\r\\end{align}\\]\nWhen \\(x_2 = \\pm 2e^{x_1}\\), putting this back in (5) we get:\r\\[\\begin{align}\r-3e^{2 x_1} + 4t - 1 = 0 \\nonumber \\\\\r\\implies x_1 = \\frac{1}{2} log\\left(\\frac{4t-1}{3}\\right) \\tag{8} \\\\\rwhere \\,\\,\\,\\, 1/4 \u0026lt; t \\le 1\r\\end{align}\\]\nCombining all the findings from (6), (7) and (8), we get:\r\\[\\begin{align}\rx_1(t) = \\begin{cases}\r\\frac{1}{2} log(1-4t) \u0026amp; \\text{for } 0\\le t \\le 1/4\\\\\r\\frac{1}{2} log\\left(\\frac{4t-1}{3}\\right) \u0026amp; \\text{for } 1/4 \u0026lt; t \\leq 1\r\\end{cases} \\tag{9} \\\\\rx_1(t) = \\begin{cases}\r0 \u0026amp; \\text{for } 0\\le t \\le 1/4\\\\\r\\pm 2 \\sqrt{\\left(\\frac{4t-1}{3}\\right)} \u0026amp; \\text{for } 1/4 \u0026lt; t \\leq 1\r\\end{cases} \\tag{10}\r\\end{align}\\]\nWhich gives us the solution to the system of equations in (3) at \\(t = 1\\). However, note that the functions \\(x_1(t), x_2(t)\\) are non-differentiable, which disqualifies them from being solution paths (Garcia and Zangwill 1981). This is evident in the plots shown below:\rOne could always try another formulation for \\(H(x_1, x_2, t)\\) which leads to well defined paths to the desired solution from the known solution (i.e. the solution to \\(H(x_1, x_2, 0)\\)). That is left as an exercise to the reader.\n\rReferences\rBajari, Patrick, Han Hong, John Krainer, and Denis Nekipelov. 2010. “Computing Equilibria in Static Games of Incomplete Information Using the All-Solution Homotopy.” Operations Research 58 (4-part 2).\n\rBorkovsky, Ron N., Ulrich Doraszelski, and Yaroslav Kryukov. 2010. “A user’s guide to solving dynamic stochastic games using the homotopy method.” Operations Research 58 (4 PART 2): 1116–32. https://doi.org/10.1287/opre.1100.0843.\n\rGarcia, C B, and W I Zangwill. 1981. “Pathways to solutions, fixed points, and equilibria. 1981.” Prentice-Hall, Englewood Cliffs, NJ.\n\r\r\r","date":1587859200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587952794,"objectID":"f73b968e98aaf9d7fff6d3427170c94b","permalink":"/post/the-homotopy-principle-simple-examples/","publishdate":"2020-04-26T00:00:00Z","relpermalink":"/post/the-homotopy-principle-simple-examples/","section":"post","summary":"This post presents 2 simple examples of the homotopy principle applied to (fairly easy) linear and nonlinear systems of equations. At a very abstract level, given a system of equations for which a solution is needed, we convert this system to one whose solution we already know (or is easy to find out), and then bend this system till we get the solution to the original set of equations. These methods have very broad applications (Garcia and Zangwill 1981), and have been applied in the context of finding equlibria in Static Games (Bajari et al.","tags":["Fixed Points","Equilibria"],"title":"The Homotopy Principle: Simple Examples","type":"post"},{"authors":[],"categories":["Bayesian Nonparametrics"],"content":"\rThis post is a continuation of the post that proves the additive property of the Gamma distribution. We use the same parameterization of the Gamma Distribution as before, and set the rate parameter \\(=1\\) unless otherwise specified. In this post I prove that the following property holds:\n\\[\\begin{align}\r\\frac{X_i}{\\sum_{i=1}^{N} X_i} \\sim Beta(\\alpha_i, \\sum_{j \\ne i} \\alpha_j) \\tag{1} \\\\\rWhere \\, X_i \\sim Gamma(\\alpha_i, 1), \\, \\, \\{i,j\\} \\in \\{1,2, \\dots, N\\} \\end{align}\\]\nWhere\n\\[\\begin{align}\rGamma(\\alpha_i, 1) = f_{X_i}(x; \\alpha_i) = \\frac{1}{\\Gamma(\\alpha_i)} x^{(\\alpha_i -1)} e^{-x} \\tag{2} \\\\\rBeta(\\alpha_i, \\sum_{j \\ne i} \\alpha_j) = f_{Y}(y; \\{\\alpha_i\\}_{i=1,\\dots,N}) = \\frac{\\Gamma(\\sum_{i=1}^{N} \\alpha_i)}{\\Gamma(\\alpha_i) \\Gamma(\\sum_{j \\ne i} \\alpha_j)} y^{\\alpha_i - 1} (1-y)^{(\\sum_{j \\ne i} \\alpha_j - 1)}\r\\tag{3}\r\\end{align}\\]\nImportantly, the \\(X_i\\)’s are independent. Let’s attempt to prove this for a small value of \\(N\\). For \\(N=1\\), the fraction in (1) is no longer random. For \\(N=2\\), we derive the property in (1) from first principles. Let \\(Y = \\frac{X_1}{X_1 + X_2}\\) where \\(X_1\\) and \\(X_2\\) are independent, Gamma distributed variables1. We try to obtain the distribution of \\(Y\\):\n\\[\\begin{align}\rP(Y \\le y) \u0026amp; = P(\\frac{X_1}{X_1 + X_2} \\le y) \\nonumber \\\\\r\\implies P(Y \\le y) \u0026amp; = P(X_1 \\le y(X_1 + X_2)) \\nonumber \\\\\r\\implies P(Y \\le y) \u0026amp; = P(X_1 (1-y) \\le y X_2)) \\nonumber \\\\\r\\implies P(Y \\le y) \u0026amp; = P(X_1 \\le \\frac{y}{1-y} X_2)) \\tag{4} \\end{align}\\]\nUsing similar logic from the previous post, we know that the expression in (4) is a double integral, one to vary \\(X_2\\) over the range of values it can possibly take: \\((0, \\infty)\\), and the other to vary \\(X_1\\) from \\((0, \\frac{y}{1-y} X_2)\\). This integral takes the form below:\n\\[\\begin{align}\rP(Y \\le y) \u0026amp; = P(X_1 \\le \\frac{y}{1-y} X_2)) = \\int_{0}^{\\infty} \\left( \\int_{0}^{\\frac{y}{1-y} X_2} \\frac{1}{\\Gamma(\\alpha_1)} x_{1}^{(\\alpha_1 -1)} e^{-x_1} dX_1 \\right) \\frac{1}{\\Gamma(\\alpha_2)} x_{2}^{(\\alpha_2 -1)} e^{-x_2} dX_2 \\tag{5} \\end{align}\\]\nNote also, that the density of \\(Y\\) is given by:\r\\[\\begin{align}\rf_{Y}(y) \u0026amp; = \\frac{dP(Y \\le y)}{dy} \\tag{6} \\end{align}\\]\nApplying the identity in (6) to the integral in (5) and using the Leibniz integral rule, we get the following:\n\\[\\begin{align}\rf_{Y}(y) \u0026amp; = \\frac{d}{dy} \\int_{0}^{\\infty} \\left( \\int_{0}^{\\frac{y}{1-y} X_2} \\frac{1}{\\Gamma(\\alpha_1)} x_{1}^{(\\alpha_1 -1)} e^{-x_1} dX_1 \\right) \\frac{1}{\\Gamma(\\alpha_2)} x_{2}^{(\\alpha_2 -1)} e^{-x_2} dX_2 \\nonumber \\\\\r\\implies f_{Y}(y) \u0026amp; = \\int_{0}^{\\infty} \\left( \\frac{1}{\\Gamma(\\alpha_1)} \\left( \\frac{y}{1-y} x_2 \\right)^{(\\alpha_1 -1)} \\frac{e^{-(\\frac{y}{1-y} x_2)}}{(1-y)^2} x_2 \\right) \\frac{1}{\\Gamma(\\alpha_2)} x_{2}^{(\\alpha_2 -1)} e^{-x_2} dX_2 \\nonumber \\\\\r\\implies f_{Y}(y) \u0026amp; = \\frac{y^{\\alpha_1 - 1} \\int_{0}^{\\infty} \\left( x_2^{\\alpha_1-1} e^{-(\\frac{y}{1-y} x_2)} x_2 \\right) x_{2}^{(\\alpha_2 -1)} e^{-x_2} dX_2}{\\Gamma(\\alpha_1) \\Gamma(\\alpha_2) (1-y)^{\\alpha_1 - 1}(1-y)^2} \\nonumber \\\\\r\\implies f_{Y}(y) \u0026amp; = \\frac{y^{\\alpha_1 - 1}}{\\Gamma(\\alpha_1) \\Gamma(\\alpha_2) (1-y)^{\\alpha_1 + 1}} \\int_{0}^{\\infty} x_{2}^{(\\alpha_1 + \\alpha_2 -1)} e^{-(\\frac{x_2}{1-y})} dX_2 \\tag{7} \\end{align}\\]\nWe now attempt to convert the integral in (7) into a form that is well known. We set \\(Z = \\frac{X_2}{1-y}\\), which gives us \\(dX_2 = (1-y)dZ\\), and also note that \\(Z\\) can take the range of values bounded by the interval \\((0,\\infty)\\) (\\(Z \\rightarrow 0\\) as \\(X_2 \\rightarrow 0\\) and \\(Z \\rightarrow \\infty\\) as \\(X_2 \\rightarrow \\infty\\), since \\(Y \\rightarrow 0\\)). Replacing \\(X_2\\) with \\(Z\\) in (7), we get:\n\\[\\begin{align}\rf_{Y}(y) \u0026amp; = \\frac{y^{\\alpha_1 - 1}}{\\Gamma(\\alpha_1) \\Gamma(\\alpha_2) (1-y)^{\\alpha_1 + 1}} \\int_{0}^{\\infty} ((1-y)z)^{(\\alpha_1 + \\alpha_2 -1)} e^{-z} (1-y) dz \\nonumber \\\\\r\\implies f_{Y}(y) \u0026amp; = \\frac{y^{\\alpha_1 - 1} (1-y)^{(\\alpha_1 + \\alpha_2)}}{\\Gamma(\\alpha_1) \\Gamma(\\alpha_2) (1-y)^{\\alpha_1 + 1}} \\int_{0}^{\\infty} z^{(\\alpha_1 + \\alpha_2 -1)} e^{-z} dz \\nonumber \\\\\r\\implies f_{Y}(y) \u0026amp; = \\frac{y^{\\alpha_1 - 1} (1-y)^{\\alpha_2 - 1}}{\\Gamma(\\alpha_1) \\Gamma(\\alpha_2)} \\int_{0}^{\\infty} z^{(\\alpha_1 + \\alpha_2 -1)} e^{-z} dz \\tag{8} \\end{align}\\]\nThe integral in (8) is the Gamma function, whose value is given below:\n\\[\\begin{align}\r\\int_{0}^{\\infty} z^{(\\alpha_1 + \\alpha_2 -1)} e^{-z} dz = \\Gamma(\\alpha_1 + \\alpha_2) \\tag{9} \\end{align}\\]\nSubstituting (9) back into (8), we get:\r\\[\\begin{align}\rf_{Y}(y) \u0026amp; = \\frac{y^{\\alpha_1 - 1} (1-y)^{\\alpha_2 - 1}}{\\Gamma(\\alpha_1) \\Gamma(\\alpha_2)} \\Gamma(\\alpha_1 + \\alpha_2) \\nonumber \\\\\r\\implies f_{Y}(y) \u0026amp; = \\frac{\\Gamma(\\alpha_1 + \\alpha_2)}{\\Gamma(\\alpha_1) \\Gamma(\\alpha_2)} y^{\\alpha_1 - 1} (1-y)^{\\alpha_2 - 1} \\tag{10} \\\\\r\\implies f_{Y}(y) \u0026amp; = Beta(\\alpha_1, \\alpha_2) \\nonumber\r\\end{align}\\]\nFrom (10), we have proved that the property in (1) holds for \\(N=2\\). We now have to prove this for any general \\(N\\). Now, let \\(Z = \\sum_{i=2}^{N} X_i\\) and \\(Y = \\frac{X_1}{\\sum_{i=1}^{N} X_i}\\), which gives us the following identities:\n\\[\\begin{align}\r\\sum_{i=1}^{N} X_i \u0026amp; = X_{1} + Z \\tag{11} \\\\\r\\implies Y \u0026amp; = \\frac{X_1}{\\sum_{i=1}^{N} X_i} = \\frac{X_1}{X_1 + Z} \\tag{12} \\\\\rZ \u0026amp; \\sim Gamma(\\sum_{i=2}^{N} \\alpha_i,1) \\tag{13}\r\\end{align}\\]\nWhere the result in (13) was proved in the previous post. We see that the form of \\(Y\\) from (12) is similar to that of \\(Y\\) in the \\(N=2\\) case. Applying the result for \\(N=2\\) to (12), we get:\n\\[\\begin{align}\rY \u0026amp; = \\frac{X_1}{X_1 + Z} \\sim Beta(\\alpha_1, \\sum_{i=2}^{N} \\alpha_i) \\tag{14} \\\\\r\\implies f_{Y}(y) \u0026amp; = \\frac{\\Gamma(\\alpha_1 + \\sum_{i=2}^{N} \\alpha_i)}{\\Gamma(\\alpha_1) \\Gamma(\\sum_{i=2}^{N} \\alpha_i)} y^{\\alpha_1 - 1} (1-y)^{\\sum_{i=2}^{N} \\alpha_i - 1} \\nonumber \\\\\r\\implies f_{Y}(y) \u0026amp; = \\frac{\\Gamma(\\sum_{i=1}^{N} \\alpha_i)}{\\Gamma(\\alpha_1) \\Gamma(\\sum_{i=2}^{N} \\alpha_i)} y^{\\alpha_1 - 1} (1-y)^{\\sum_{i=2}^{N} \\alpha_i - 1} \\tag{15}\r\\end{align}\\]\nWhich is what we set out to prove.\n\rWithout loss of generality, I prove this property for \\(i=1\\)↩\n\r\r\r","date":1567814400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567864670,"objectID":"e70bdde3977b801e47a729564ada64a9","permalink":"/post/deriving-the-beta-distribution-from-gamma-distributed-variables/","publishdate":"2019-09-07T00:00:00Z","relpermalink":"/post/deriving-the-beta-distribution-from-gamma-distributed-variables/","section":"post","summary":"This post is a continuation of the post that proves the additive property of the Gamma distribution. We use the same parameterization of the Gamma Distribution as before, and set the rate parameter \\(=1\\) unless otherwise specified. In this post I prove that the following property holds:\n\\[\\begin{align}\r\\frac{X_i}{\\sum_{i=1}^{N} X_i} \\sim Beta(\\alpha_i, \\sum_{j \\ne i} \\alpha_j) \\tag{1} \\\\\rWhere \\, X_i \\sim Gamma(\\alpha_i, 1), \\, \\, \\{i,j\\} \\in \\{1,2, \\dots, N\\} \\end{align}\\]","tags":["Bayesian Nonparametrics","Gamma Distribution","Beta Distribution"],"title":"Deriving the Beta Distribution from Gamma Distributed Variables","type":"post"},{"authors":[],"categories":["Bayesian Nonparametrics"],"content":"\rWhile learning Bayesian Nonparametric methodology, I found a few properties useful in understanding the way Dirichlet Process Priors worked (this is a prior used very commonly in the field (Ferguson (1973), Antoniak (1974))) . This post will be one in a series that describes each of the aforementioned properties and provides simple proofs/examples of the same.\nAs the title suggests, we start with the Gamma distribution. We use the shape and rate parameterization of the distribution (as explained here). Without loss of generality in this case, we also set the rate parameter $ = 1$ for all random variables unless otherwise mentioned. We want to prove the additive property of the gamma distribution:\n\\[\\begin{align}\r\\sum_{i=1}^{N} X_i \\sim Gamma(\\sum_{i=1}^{N}\\alpha_i, 1) \\nonumber \\\\\rWhere \\, X_i \\sim Gamma(\\alpha_i, 1), \\, \\, i \\in \\{1,2, \\dots, N\\} \\tag{1} \\end{align}\\]\nWhere\r\\[\\begin{align}\rGamma(\\alpha_i, 1) = f_{X_i}(x; \\alpha_i) = \\frac{1}{\\Gamma(\\alpha_i)} x^{(\\alpha_i -1)} e^{-x} \\tag{2} \\end{align}\\]\nImportantly, the \\(X_i\\)’s are independent. We prove the identity by induction. For \\(N = 1\\), the property in (1) holds trivially. For \\(N=2\\), we set about deriving this from first principles. Let \\(Y = X_1 + X_2\\). We try to get the distribution of \\(Y\\). The following holds for \\(Y\\):\n\\[\\begin{align}\rP(Y \\le y) \u0026amp; = P(X_1 + X_2 \\le y) \\nonumber \\\\\r\\implies P(Y \\le y) \u0026amp; = P(X_1 \\le y - X_2) \\tag{3} \\end{align}\\]\nIf we knew the value of \\(X_2\\), the expression in (3) is obtained by integrating the probability density of \\(X_1\\) over \\((0, y-X_2)\\), given below:\n\\[\\begin{align}\rP(X_1 \\le y-X_2|X_2 = x_2) \u0026amp; = \\int_{0}^{y-x_2} \\frac{1}{\\Gamma(\\alpha_1)} x_{1}^{(\\alpha_1 -1)} e^{-x_1} dX_1 = F(y, X_2; \\alpha_1) \\tag{4} \\end{align}\\]\nHowever, since \\(X_2\\) is a random variable varying from \\((0, \\infty)\\), the value of the expression in (3) is the expectation of \\(F(y, X_2; \\alpha_1)\\) with respect to \\(X_2\\), given below:\n\\[\\begin{align}\rP(Y \\le y) \u0026amp; = \\int_{0}^{\\infty} \\left[ F(y, X_2;\\alpha_1) \\right] \\frac{1}{\\Gamma(\\alpha_2)} x_{2}^{(\\alpha_2 -1)} e^{-x_2} dX_2 \\tag{5} \\end{align}\\]\nThe expression in (5) is a double integral, one to vary \\(X_2\\) over the range of values it can possibly take: \\((0, \\infty)\\), and the other to vary \\(X_1\\) from \\((0, y-X_2)\\). Putting (4) in (5), we get:\n\\[\\begin{align}\rP(Y \\le y) \u0026amp; = \\int_{0}^{\\infty} \\left[ \\int_{0}^{y-x_2} \\frac{1}{\\Gamma(\\alpha_1)} x_{1}^{(\\alpha_1 -1)} e^{-x_1} dX_1 \\right] \\frac{1}{\\Gamma(\\alpha_2)} x_{2}^{(\\alpha_2 -1)} e^{-x_2} dX_2 \\tag{6} \\end{align}\\]\nNote also, that the density of \\(Y\\) is given by:\r\\[\\begin{align}\rf_{Y}(y) \u0026amp; = \\frac{dP(Y \\le y)}{dy} \\tag{7} \\end{align}\\]\nApplying the identity in (7) to the integral in (6) and using the Leibniz integral rule, we get the following:\n\\[\\begin{align}\rf_{Y}(y) \u0026amp; = \\frac{d}{dy} \\int_{0}^{\\infty} \\left( \\int_{0}^{y-x_2} \\frac{1}{\\Gamma(\\alpha_1)} x_{1}^{(\\alpha_1 -1)} e^{-x_1} dX_1 \\right) \\frac{1}{\\Gamma(\\alpha_2)} x_{2}^{(\\alpha_2 -1)} e^{-x_2} dX_2 \\nonumber \\\\\r\\implies f_{Y}(y) \u0026amp; = \\int_{0}^{\\infty} \\left( \\frac{1}{\\Gamma(\\alpha_1)} (y-x_2)^{(\\alpha_1 -1)} e^{-(y-x_2)} \\right) \\frac{1}{\\Gamma(\\alpha_2)} x_{2}^{(\\alpha_2 -1)} e^{-x_2} dX_2 \\nonumber \\\\\r\\implies f_{Y}(y) \u0026amp; = \\int_{0}^{\\infty} \\frac{1}{\\Gamma(\\alpha_1)} \\frac{1}{\\Gamma(\\alpha_2)} (y-x_2)^{(\\alpha_1 -1)} x_{2}^{(\\alpha_2 -1)} e^{-y} dX_2 \\tag{8} \\end{align}\\]\nWe now attempt to convert the integral in (8) into a form that is well known. We set \\(Z = \\frac{X_2}{y}\\), which gives us \\(dX_2 = ydZ\\), and also note that \\(Z\\) can take the range of values bounded by the interval \\((0,1)\\) (\\(Z \\rightarrow 0\\) as \\(X_2 \\rightarrow 0\\) and \\(Z \\rightarrow 1\\) as \\(X_2 \\rightarrow \\infty\\), since \\(Y \\rightarrow \\infty\\) too). Replacing \\(X_2\\) with \\(Z\\) in (8), we get:\n\\[\\begin{align}\rf_{Y}(y) \u0026amp; = \\int_{0}^{1} \\frac{1}{\\Gamma(\\alpha_1)} \\frac{1}{\\Gamma(\\alpha_2)} (y- yz)^{(\\alpha_1 -1)} (yz)^{(\\alpha_2 -1)} e^{-y} ydZ \\nonumber \\\\\r\\implies f_{Y}(y) \u0026amp; = \\int_{0}^{1} \\frac{1}{\\Gamma(\\alpha_1)} \\frac{1}{\\Gamma(\\alpha_2)} y^{(\\alpha_1 -1)} (1- z)^{(\\alpha_1 -1)} y^{(\\alpha_2 -1)} z^{(\\alpha_2 -1)} e^{-y} ydZ \\nonumber \\\\\r\\implies f_{Y}(y) \u0026amp; = \\int_{0}^{1} \\frac{1}{\\Gamma(\\alpha_1)} \\frac{1}{\\Gamma(\\alpha_2)} y^{(\\alpha_1 + \\alpha_2 -1)} e^{-y} (1- z)^{(\\alpha_1 -1)} z^{(\\alpha_2 -1)} dZ \\nonumber \\\\\r\\implies f_{Y}(y) \u0026amp; = \\frac{1}{\\Gamma(\\alpha_1)} \\frac{1}{\\Gamma(\\alpha_2)} y^{(\\alpha_1 + \\alpha_2 -1)} e^{-y} \\int_{0}^{1} (1- z)^{(\\alpha_1 -1)} z^{(\\alpha_2 -1)} dZ \\tag{9} \\end{align}\\]\nThe integral in (9) is the Beta function, whose value is given below:\n\\[\\begin{align}\r\\int_{0}^{1} (1- z)^{(\\alpha_1 -1)} z^{(\\alpha_2 -1)} dZ = \\frac{\\Gamma(\\alpha_1)\\Gamma(\\alpha_2)}{\\Gamma(\\alpha_1 + \\alpha_2)} \\tag{10}\r\\end{align}\\]\nSubstituting (10) back into (9), we get:\r\\[\\begin{align}\rf_{Y}(y) \u0026amp; = \\frac{1}{\\Gamma(\\alpha_1)} \\frac{1}{\\Gamma(\\alpha_2)} y^{(\\alpha_1 + \\alpha_2 -1)} e^{-y} \\frac{\\Gamma(\\alpha_1)\\Gamma(\\alpha_2)}{\\Gamma(\\alpha_1 + \\alpha_2)}\\nonumber \\\\\r\\implies f_{Y}(y) \u0026amp; = \\frac{1}{\\Gamma(\\alpha_1 + \\alpha_2)} y^{(\\alpha_1 + \\alpha_2 -1)} e^{-y} \\tag{11} \\\\\r\\implies f_{Y}(y) \u0026amp; = Gamma(\\alpha_1 + \\alpha_2, 1) \\nonumber\r\\end{align}\\]\nFrom (11), we have now proved that the property in (1) holds for \\(N=2\\). We now assume this is true for \\(N\\) and show that this property holds for \\(N+1\\). Now, let \\(Y = \\sum_{i=1}^{N} X_i\\), which gives us the following identities:\n\\[\\begin{align}\r\\sum_{i=1}^{N+1} X_i \u0026amp; = Y + X_{N+1} \\tag{12} \\\\\rY \u0026amp; \\sim Gamma(\\sum_{i=1}^{N} \\alpha_i,1) \\tag{13}\r\\end{align}\\]\n(13) follows from the induction assumption. Since the property in (1) is true for \\(N=2\\), we combine this with the identities in (12) \u0026amp; (13) to get:\n\\[\\begin{align}\r\\sum_{i=1}^{N+1} X_i \u0026amp; = Y + X_{N+1} \\sim Gamma(\\sum_{i=1}^{N} \\alpha_i + \\alpha_{N+1},1) \\nonumber \\\\\r\\implies \\sum_{i=1}^{N+1} X_i \u0026amp; \\sim Gamma(\\sum_{i=1}^{N+1} \\alpha_i,1) \\tag{14} \\end{align}\\]\n(14) shows that the property in (1) is true for \\(N+1\\), completing the proof.\nReferences\rAntoniak, Charles E. 1974. “Mixtures of Dirichlet Processes with Applications to Bayesian Nonparametric Problems.” The Annals of Statistics, 1152–74.\n\rFerguson, Thomas S. 1973. “A Bayesian Analysis of Some Nonparametric Problems.” The Annals of Statistics, 209–30.\n\r\r\r","date":1567468800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567558547,"objectID":"f0081b01add0652d81ba1d39b4bb0b10","permalink":"/post/additive-property-of-the-gamma-distribution/","publishdate":"2019-09-03T00:00:00Z","relpermalink":"/post/additive-property-of-the-gamma-distribution/","section":"post","summary":"While learning Bayesian Nonparametric methodology, I found a few properties useful in understanding the way Dirichlet Process Priors worked (this is a prior used very commonly in the field (Ferguson (1973), Antoniak (1974))) . This post will be one in a series that describes each of the aforementioned properties and provides simple proofs/examples of the same.\nAs the title suggests, we start with the Gamma distribution. We use the shape and rate parameterization of the distribution (as explained here).","tags":["Bayesian Nonparametrics","Gamma Distribution"],"title":"Additive Property of the Gamma Distribution","type":"post"},{"authors":[],"categories":["Scientific Computing"],"content":"\rI recently read a paper (Heiss and Winschel 2008) that advocated the use of certain techniques (Sparse Grids, SG henceforth) in numerical integration to calculate likelihood functions, as opposed to using Monte Carlo (MC henceforth) methods for the same. While approximating integrals with MC methods are simpler to implement, they might lead to integral values with considerable simulation error (Skrainka and Judd 2011). This post attempts to demonstrate the claim in Skrainka and Judd (2011) using two very simple integrals, to which we already know the value. I attempt to compare the outcomes from using MC and SG.\nThe integrals I’ll be evaluating are:\n\\[\\begin{equation}\r\\int_{-\\infty}^{\\infty} \\left( \\sum_{i=1}^{5} x_i \\right) dF_{X} \\tag{1} \\end{equation}\\]\nand\r\\[\\begin{equation}\r\\int_{-\\infty}^{\\infty} \\left( \\prod_{i=1}^{5} x_i^2 \\right) dF_{X} \\tag{2}\r\\end{equation}\\]\nwhere \\(X = \\{x_i\\}_{i=1}^{5}\\) is a five dimensional random variable, which is distributed according to the multivariate standard normal:\r\\[\rX \\sim N\\left( \\left[ \\begin{array}\r{r}\r0 \\\\\r0 \\\\\r0 \\\\\r0 \\\\\r0 \\\\\r\\end{array}\\right], \\left[ \\begin{array}\r{rrrrr}\r1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\\r0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\\r0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\\\\r0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\\r0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\\\\r\\end{array}\\right] \\right)\r\\]\nGiven the distribution of \\(X\\), the values of the integrals above are easily obtained from standard results (the value of (1) is \\(0\\) and that of (2) is \\(1\\)) respectively.\nI write some utility functions in R to compute the integrands above:\n#function to compute the sum of components of the random vector\rs \u0026lt;- function(x)\r{\rreturn(sum(x))\r}\r#function to compute the square product of the components of the random vector\rp \u0026lt;- function(x)\r{\rreturn(prod(x^2))\r}\rI now write a function that:\n\rSimulates a certain number of draws from the distribution of the random variable \\(X\\)\rComputes the integrand function using each of these draws as input\rTakes the average of the values computed in the previous step\r\rThis function, in effect, would give us the approximate value of the integral via MC methodology.\nThe code is provided below, note that I use the mvtnorm package to create random draws.\nlibrary(mvtnorm)\r#Function to calculate the MC approximation for the integral\rmc_int \u0026lt;- function(s, n, mu, sigma)\r{\r#generate random draws\rx \u0026lt;- rmvnorm(n, mean = mu, sigma = sigma)\r#now get the integral\rmc_int_n \u0026lt;- mean(apply(x, 1, s))\rreturn(mc_int_n)\r}\rset.seed(100)\rn \u0026lt;- 1000\rmc_val \u0026lt;- mc_int(s, n, mu = rep(0,5), sigma = diag(5))\rmc_val\r## [1] 0.007150433\rThe result, 0.00715 is not far off from the true value of \\(0\\) at first glance, however, we need to compare this to the result from the SG approach.\nR has a package that generates sparse grids for numerical integration as described in Heiss and Winschel (2008), called SparseGrid. We now use the nodes and weights generated from this package to approximate the first integral.\rI re-use some of the code provided in the documentation for the SparseGrid package in R.\nlibrary(SparseGrid)\r#generate sparse grids for a 5 dimensional RV with accuracy level 2\rsg \u0026lt;- createSparseGrid(type=\u0026#39;KPN\u0026#39;, dimension=5, k=2)\rsg_int \u0026lt;- function(func, sg, ...)\r{\rgx \u0026lt;- apply(sg$nodes, 1, function(x) {func(x, ...)})\rreturn(sum(gx * sg$weights))\r}\rsg_val \u0026lt;- sg_int(s, sg)\rsg_val\r## [1] 0\rThe result here is exactly 0. In light of this finding, the value obtained from the MC approach, in comparison, is a little off, and tends to show a high variance in output:\nset.seed(100)\rmc_int(s, n, mu = rep(0,5), sigma = diag(5))\r## [1] 0.007150433\rmc_int(s, n, mu = rep(0,5), sigma = diag(5))\r## [1] 0.03162326\rmc_int(s, n, mu = rep(0,5), sigma = diag(5))\r## [1] -0.1287932\rmc_int(s, n, mu = rep(0,5), sigma = diag(5))\r## [1] 0.01040134\rmc_int(s, n, mu = rep(0,5), sigma = diag(5))\r## [1] -0.03798655\rIn the third case, there is a \\(-12\\%\\) error(!) in the value of the computed integral when compared to the result from the SG approach. The SG approach, in addition, shows no such variation in repeated runs, since the grid values and weights are fixed for a given accuracy level and dimension (of the variable being integrated).\nI repeat the calculations for the second integral, as shown below\nMC approach:\nset.seed(100)\rn \u0026lt;- 1000\rmc_val \u0026lt;- mc_int(p, n, mu = rep(0,5), sigma = diag(5))\rmc_val\r## [1] 1.001089\rSG approach:\n#generate sparse grids for a 5 dimensional RV with accuracy level 6\rsg \u0026lt;- createSparseGrid(type=\u0026#39;KPN\u0026#39;, dimension=5, k=6)\rsg_val \u0026lt;- sg_int(p, sg)\rsg_val\r## [1] 1\rOnce again, the SG approach gives us an exact value (note that the value of \\(k\\), the accuracy level, has gone up, since the integrand is a higher order function). Again, the difference of the results between the two approaches doesn’t seem that large. However, variability of the results from the MC approach is still a concern, as shown below:\nset.seed(100)\rmc_int(p, n, mu = rep(0,5), sigma = diag(5))\r## [1] 1.001089\rmc_int(p, n, mu = rep(0,5), sigma = diag(5))\r## [1] 0.4672555\rmc_int(p, n, mu = rep(0,5), sigma = diag(5))\r## [1] 1.14692\rmc_int(p, n, mu = rep(0,5), sigma = diag(5))\r## [1] 1.062975\rmc_int(p, n, mu = rep(0,5), sigma = diag(5))\r## [1] 0.9112416\rIn the second case, there is a roughly \\(53\\%\\) (!!) error when compared to the true value of the integral. This variability could be worse with more complicated integrands.\nOne suggestion to reduce variability in MC methods is to increase the number of draws, but that would entail a lot of calculations and result in longer runtimes.\nReferences\rGenz, Alan, Frank Bretz, Tetsuhisa Miwa, Xuefei Mi, Friedrich Leisch, Fabian Scheipl, and Torsten Hothorn. 2019. mvtnorm: Multivariate Normal and T Distributions. https://CRAN.R-project.org/package=mvtnorm.\n\rHeiss, Florian, and Viktor Winschel. 2008. “Likelihood Approximation by Numerical Integration on Sparse Grids.” Journal of Econometrics 144 (1): 62–80.\n\rSkrainka, Benjamin S, and Kenneth L Judd. 2011. “High Performance Quadrature Rules: How Numerical Integration Affects a Popular Model of Product Differentiation.” Available at SSRN 1870703.\n\r\r\r","date":1564272000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564620683,"objectID":"b808947d31eeb351808114760100dc6d","permalink":"/post/numerical-integration-with-sparse-grids/","publishdate":"2019-07-28T00:00:00Z","relpermalink":"/post/numerical-integration-with-sparse-grids/","section":"post","summary":"I recently read a paper (Heiss and Winschel 2008) that advocated the use of certain techniques (Sparse Grids, SG henceforth) in numerical integration to calculate likelihood functions, as opposed to using Monte Carlo (MC henceforth) methods for the same. While approximating integrals with MC methods are simpler to implement, they might lead to integral values with considerable simulation error (Skrainka and Judd 2011). This post attempts to demonstrate the claim in Skrainka and Judd (2011) using two very simple integrals, to which we already know the value.","tags":["Numerical Integration","Sparse Grids"],"title":"Numerical Integration With Sparse Grids","type":"post"}]