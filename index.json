[{"authors":["admin"],"categories":null,"content":"I currently work as a Data Scientist and am interested in bringing together ideas from Economics, Statistics, Computer Science (and sometimes Physics) to help answer pertinent business questions.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I currently work as a Data Scientist and am interested in bringing together ideas from Economics, Statistics, Computer Science (and sometimes Physics) to help answer pertinent business questions.","tags":null,"title":"Journeyman Modeler","type":"authors"},{"authors":[],"categories":["Bayesian Nonparametrics"],"content":"\rThis post is a continuation of the post that proves the additive property of the Gamma distribution. We use the same parameterization of the Gamma Distribution as before, and set the rate parameter \\(=1\\) unless otherwise specified. In this post I prove that the following property holds:\n\\[\\begin{align}\r\\frac{X_i}{\\sum_{i=1}^{N} X_i} \\sim Beta(\\alpha_i, \\sum_{j \\ne i} \\alpha_j) \\tag{1} \\\\\rWhere \\, X_i \\sim Gamma(\\alpha_i, 1), \\, \\, \\{i,j\\} \\in \\{1,2, \\dots, N\\} \\end{align}\\]\nWhere\n\\[\\begin{align}\rGamma(\\alpha_i, 1) = f_{X_i}(x; \\alpha_i) = \\frac{1}{\\Gamma(\\alpha_i)} x^{(\\alpha_i -1)} e^{-x} \\tag{2} \\\\\rBeta(\\alpha_i, \\sum_{j \\ne i} \\alpha_j) = f_{Y}(y; \\{\\alpha_i\\}_{i=1,\\dots,N}) = \\frac{\\Gamma(\\sum_{i=1}^{N} \\alpha_i)}{\\Gamma(\\alpha_i) \\Gamma(\\sum_{j \\ne i} \\alpha_j)} y^{\\alpha_i - 1} (1-y)^{(\\sum_{j \\ne i} \\alpha_j - 1)}\r\\tag{3}\r\\end{align}\\]\nImportantly, the \\(X_i\\)’s are independent. Let’s attempt to prove this for a small value of \\(N\\). For \\(N=1\\), the fraction in (1) is no longer random. For \\(N=2\\), we derive the property in (1) from first principles. Let \\(Y = \\frac{X_1}{X_1 + X_2}\\) where \\(X_1\\) and \\(X_2\\) are independent, Gamma distributed variables1. We try to obtain the distribution of \\(Y\\):\n\\[\\begin{align}\rP(Y \\le y) \u0026amp; = P(\\frac{X_1}{X_1 + X_2} \\le y) \\nonumber \\\\\r\\implies P(Y \\le y) \u0026amp; = P(X_1 \\le y(X_1 + X_2)) \\nonumber \\\\\r\\implies P(Y \\le y) \u0026amp; = P(X_1 (1-y) \\le y X_2)) \\nonumber \\\\\r\\implies P(Y \\le y) \u0026amp; = P(X_1 \\le \\frac{y}{1-y} X_2)) \\tag{4} \\end{align}\\]\nUsing similar logic from the previous post, we know that the expression in (4) is a double integral, one to vary \\(X_2\\) over the range of values it can possibly take: \\((0, \\infty)\\), and the other to vary \\(X_1\\) from \\((0, \\frac{y}{1-y} X_2)\\). This integral takes the form below:\n\\[\\begin{align}\rP(Y \\le y) \u0026amp; = P(X_1 \\le \\frac{y}{1-y} X_2)) = \\int_{0}^{\\infty} \\left( \\int_{0}^{\\frac{y}{1-y} X_2} \\frac{1}{\\Gamma(\\alpha_1)} x_{1}^{(\\alpha_1 -1)} e^{-x_1} dX_1 \\right) \\frac{1}{\\Gamma(\\alpha_2)} x_{2}^{(\\alpha_2 -1)} e^{-x_2} dX_2 \\tag{5} \\end{align}\\]\nNote also, that the density of \\(Y\\) is given by:\r\\[\\begin{align}\rf_{Y}(y) \u0026amp; = \\frac{dP(Y \\le y)}{dy} \\tag{6} \\end{align}\\]\nApplying the identity in (6) to the integral in (5) and using the Leibniz integral rule, we get the following:\n\\[\\begin{align}\rf_{Y}(y) \u0026amp; = \\frac{d}{dy} \\int_{0}^{\\infty} \\left( \\int_{0}^{\\frac{y}{1-y} X_2} \\frac{1}{\\Gamma(\\alpha_1)} x_{1}^{(\\alpha_1 -1)} e^{-x_1} dX_1 \\right) \\frac{1}{\\Gamma(\\alpha_2)} x_{2}^{(\\alpha_2 -1)} e^{-x_2} dX_2 \\nonumber \\\\\r\\implies f_{Y}(y) \u0026amp; = \\int_{0}^{\\infty} \\left( \\frac{1}{\\Gamma(\\alpha_1)} \\left( \\frac{y}{1-y} x_2 \\right)^{(\\alpha_1 -1)} \\frac{e^{-(\\frac{y}{1-y} x_2)}}{(1-y)^2} x_2 \\right) \\frac{1}{\\Gamma(\\alpha_2)} x_{2}^{(\\alpha_2 -1)} e^{-x_2} dX_2 \\nonumber \\\\\r\\implies f_{Y}(y) \u0026amp; = \\frac{y^{\\alpha_1 - 1} \\int_{0}^{\\infty} \\left( x_2^{\\alpha_1-1} e^{-(\\frac{y}{1-y} x_2)} x_2 \\right) x_{2}^{(\\alpha_2 -1)} e^{-x_2} dX_2}{\\Gamma(\\alpha_1) \\Gamma(\\alpha_2) (1-y)^{\\alpha_1 - 1}(1-y)^2} \\nonumber \\\\\r\\implies f_{Y}(y) \u0026amp; = \\frac{y^{\\alpha_1 - 1}}{\\Gamma(\\alpha_1) \\Gamma(\\alpha_2) (1-y)^{\\alpha_1 + 1}} \\int_{0}^{\\infty} x_{2}^{(\\alpha_1 + \\alpha_2 -1)} e^{-(\\frac{x_2}{1-y})} dX_2 \\tag{7} \\end{align}\\]\nWe now attempt to convert the integral in (7) into a form that is well known. We set \\(Z = \\frac{X_2}{1-y}\\), which gives us \\(dX_2 = (1-y)dZ\\), and also note that \\(Z\\) can take the range of values bounded by the interval \\((0,\\infty)\\) (\\(Z \\rightarrow 0\\) as \\(X_2 \\rightarrow 0\\) and \\(Z \\rightarrow \\infty\\) as \\(X_2 \\rightarrow \\infty\\), since \\(Y \\rightarrow 0\\)). Replacing \\(X_2\\) with \\(Z\\) in (7), we get:\n\\[\\begin{align}\rf_{Y}(y) \u0026amp; = \\frac{y^{\\alpha_1 - 1}}{\\Gamma(\\alpha_1) \\Gamma(\\alpha_2) (1-y)^{\\alpha_1 + 1}} \\int_{0}^{\\infty} ((1-y)z)^{(\\alpha_1 + \\alpha_2 -1)} e^{-z} (1-y) dz \\nonumber \\\\\r\\implies f_{Y}(y) \u0026amp; = \\frac{y^{\\alpha_1 - 1} (1-y)^{(\\alpha_1 + \\alpha_2)}}{\\Gamma(\\alpha_1) \\Gamma(\\alpha_2) (1-y)^{\\alpha_1 + 1}} \\int_{0}^{\\infty} z^{(\\alpha_1 + \\alpha_2 -1)} e^{-z} dz \\nonumber \\\\\r\\implies f_{Y}(y) \u0026amp; = \\frac{y^{\\alpha_1 - 1} (1-y)^{\\alpha_2 - 1}}{\\Gamma(\\alpha_1) \\Gamma(\\alpha_2)} \\int_{0}^{\\infty} z^{(\\alpha_1 + \\alpha_2 -1)} e^{-z} dz \\tag{8} \\end{align}\\]\nThe integral in (8) is the Gamma function, whose value is given below:\n\\[\\begin{align}\r\\int_{0}^{\\infty} z^{(\\alpha_1 + \\alpha_2 -1)} e^{-z} dz = \\Gamma(\\alpha_1 + \\alpha_2) \\tag{9} \\end{align}\\]\nSubstituting (9) back into (8), we get:\r\\[\\begin{align}\rf_{Y}(y) \u0026amp; = \\frac{y^{\\alpha_1 - 1} (1-y)^{\\alpha_2 - 1}}{\\Gamma(\\alpha_1) \\Gamma(\\alpha_2)} \\Gamma(\\alpha_1 + \\alpha_2) \\nonumber \\\\\r\\implies f_{Y}(y) \u0026amp; = \\frac{\\Gamma(\\alpha_1 + \\alpha_2)}{\\Gamma(\\alpha_1) \\Gamma(\\alpha_2)} y^{\\alpha_1 - 1} (1-y)^{\\alpha_2 - 1} \\tag{10} \\\\\r\\implies f_{Y}(y) \u0026amp; = Beta(\\alpha_1, \\alpha_2) \\nonumber\r\\end{align}\\]\nFrom (10), we have proved that the property in (1) holds for \\(N=2\\). We now have to prove this for any general \\(N\\). Now, let \\(Z = \\sum_{i=2}^{N} X_i\\) and \\(Y = \\frac{X_1}{\\sum_{i=1}^{N} X_i}\\), which gives us the following identities:\n\\[\\begin{align}\r\\sum_{i=1}^{N} X_i \u0026amp; = X_{1} + Z \\tag{11} \\\\\r\\implies Y \u0026amp; = \\frac{X_1}{\\sum_{i=1}^{N} X_i} = \\frac{X_1}{X_1 + Z} \\tag{12} \\\\\rZ \u0026amp; \\sim Gamma(\\sum_{i=2}^{N} \\alpha_i,1) \\tag{13}\r\\end{align}\\]\nWhere the result in (13) was proved in the previous post. We see that the form of \\(Y\\) from (12) is similar to that of \\(Y\\) in the \\(N=2\\) case. Applying the result for \\(N=2\\) to (12), we get:\n\\[\\begin{align}\rY \u0026amp; = \\frac{X_1}{X_1 + Z} \\sim Beta(\\alpha_1, \\sum_{i=2}^{N} \\alpha_i) \\tag{14} \\\\\r\\implies f_{Y}(y) \u0026amp; = \\frac{\\Gamma(\\alpha_1 + \\sum_{i=2}^{N} \\alpha_i)}{\\Gamma(\\alpha_1) \\Gamma(\\sum_{i=2}^{N} \\alpha_i)} y^{\\alpha_1 - 1} (1-y)^{\\sum_{i=2}^{N} \\alpha_i - 1} \\nonumber \\\\\r\\implies f_{Y}(y) \u0026amp; = \\frac{\\Gamma(\\sum_{i=1}^{N} \\alpha_i)}{\\Gamma(\\alpha_1) \\Gamma(\\sum_{i=2}^{N} \\alpha_i)} y^{\\alpha_1 - 1} (1-y)^{\\sum_{i=2}^{N} \\alpha_i - 1} \\tag{15}\r\\end{align}\\]\nWhich is what we set out to prove.\n\rWithout loss of generality, I prove this property for \\(i=1\\)↩\n\r\r\r","date":1567814400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567864670,"objectID":"e70bdde3977b801e47a729564ada64a9","permalink":"/post/deriving-the-beta-distribution-from-gamma-distributed-variables/","publishdate":"2019-09-07T00:00:00Z","relpermalink":"/post/deriving-the-beta-distribution-from-gamma-distributed-variables/","section":"post","summary":"This post is a continuation of the post that proves the additive property of the Gamma distribution. We use the same parameterization of the Gamma Distribution as before, and set the rate parameter \\(=1\\) unless otherwise specified. In this post I prove that the following property holds:\n\\[\\begin{align}\r\\frac{X_i}{\\sum_{i=1}^{N} X_i} \\sim Beta(\\alpha_i, \\sum_{j \\ne i} \\alpha_j) \\tag{1} \\\\\rWhere \\, X_i \\sim Gamma(\\alpha_i, 1), \\, \\, \\{i,j\\} \\in \\{1,2, \\dots, N\\} \\end{align}\\]","tags":["Bayesian Nonparametrics","Gamma Distribution","Beta Distribution"],"title":"Deriving the Beta Distribution from Gamma Distributed Variables","type":"post"},{"authors":[],"categories":["Bayesian Nonparametrics"],"content":"\rWhile learning Bayesian Nonparametric methodology, I found a few properties useful in understanding the way Dirichlet Process Priors worked (this is a prior used very commonly in the field (Ferguson (1973), Antoniak (1974))) . This post will be one in a series that describes each of the aforementioned properties and provides simple proofs/examples of the same.\nAs the title suggests, we start with the Gamma distribution. We use the shape and rate parameterization of the distribution (as explained here). Without loss of generality in this case, we also set the rate parameter $ = 1$ for all random variables unless otherwise mentioned. We want to prove the additive property of the gamma distribution:\n\\[\\begin{align}\r\\sum_{i=1}^{N} X_i \\sim Gamma(\\sum_{i=1}^{N}\\alpha_i, 1) \\nonumber \\\\\rWhere \\, X_i \\sim Gamma(\\alpha_i, 1), \\, \\, i \\in \\{1,2, \\dots, N\\} \\tag{1} \\end{align}\\]\nWhere\r\\[\\begin{align}\rGamma(\\alpha_i, 1) = f_{X_i}(x; \\alpha_i) = \\frac{1}{\\Gamma(\\alpha_i)} x^{(\\alpha_i -1)} e^{-x} \\tag{2} \\end{align}\\]\nImportantly, the \\(X_i\\)’s are independent. We prove the identity by induction. For \\(N = 1\\), the property in (1) holds trivially. For \\(N=2\\), we set about deriving this from first principles. Let \\(Y = X_1 + X_2\\). We try to get the distribution of \\(Y\\). The following holds for \\(Y\\):\n\\[\\begin{align}\rP(Y \\le y) \u0026amp; = P(X_1 + X_2 \\le y) \\nonumber \\\\\r\\implies P(Y \\le y) \u0026amp; = P(X_1 \\le y - X_2) \\tag{3} \\end{align}\\]\nIf we knew the value of \\(X_2\\), the expression in (3) is obtained by integrating the probability density of \\(X_1\\) over \\((0, y-X_2)\\), given below:\n\\[\\begin{align}\rP(X_1 \\le y-X_2|X_2 = x_2) \u0026amp; = \\int_{0}^{y-x_2} \\frac{1}{\\Gamma(\\alpha_1)} x_{1}^{(\\alpha_1 -1)} e^{-x_1} dX_1 = F(y, X_2; \\alpha_1) \\tag{4} \\end{align}\\]\nHowever, since \\(X_2\\) is a random variable varying from \\((0, \\infty)\\), the value of the expression in (3) is the expectation of \\(F(y, X_2; \\alpha_1)\\) with respect to \\(X_2\\), given below:\n\\[\\begin{align}\rP(Y \\le y) \u0026amp; = \\int_{0}^{\\infty} \\left[ F(y, X_2;\\alpha_1) \\right] \\frac{1}{\\Gamma(\\alpha_2)} x_{2}^{(\\alpha_2 -1)} e^{-x_2} dX_2 \\tag{5} \\end{align}\\]\nThe expression in (5) is a double integral, one to vary \\(X_2\\) over the range of values it can possibly take: \\((0, \\infty)\\), and the other to vary \\(X_1\\) from \\((0, y-X_2)\\). Putting (4) in (5), we get:\n\\[\\begin{align}\rP(Y \\le y) \u0026amp; = \\int_{0}^{\\infty} \\left[ \\int_{0}^{y-x_2} \\frac{1}{\\Gamma(\\alpha_1)} x_{1}^{(\\alpha_1 -1)} e^{-x_1} dX_1 \\right] \\frac{1}{\\Gamma(\\alpha_2)} x_{2}^{(\\alpha_2 -1)} e^{-x_2} dX_2 \\tag{6} \\end{align}\\]\nNote also, that the density of \\(Y\\) is given by:\r\\[\\begin{align}\rf_{Y}(y) \u0026amp; = \\frac{dP(Y \\le y)}{dy} \\tag{7} \\end{align}\\]\nApplying the identity in (7) to the integral in (6) and using the Leibniz integral rule, we get the following:\n\\[\\begin{align}\rf_{Y}(y) \u0026amp; = \\frac{d}{dy} \\int_{0}^{\\infty} \\left( \\int_{0}^{y-x_2} \\frac{1}{\\Gamma(\\alpha_1)} x_{1}^{(\\alpha_1 -1)} e^{-x_1} dX_1 \\right) \\frac{1}{\\Gamma(\\alpha_2)} x_{2}^{(\\alpha_2 -1)} e^{-x_2} dX_2 \\nonumber \\\\\r\\implies f_{Y}(y) \u0026amp; = \\int_{0}^{\\infty} \\left( \\frac{1}{\\Gamma(\\alpha_1)} (y-x_2)^{(\\alpha_1 -1)} e^{-(y-x_2)} \\right) \\frac{1}{\\Gamma(\\alpha_2)} x_{2}^{(\\alpha_2 -1)} e^{-x_2} dX_2 \\nonumber \\\\\r\\implies f_{Y}(y) \u0026amp; = \\int_{0}^{\\infty} \\frac{1}{\\Gamma(\\alpha_1)} \\frac{1}{\\Gamma(\\alpha_2)} (y-x_2)^{(\\alpha_1 -1)} x_{2}^{(\\alpha_2 -1)} e^{-y} dX_2 \\tag{8} \\end{align}\\]\nWe now attempt to convert the integral in (8) into a form that is well known. We set \\(Z = \\frac{X_2}{y}\\), which gives us \\(dX_2 = ydZ\\), and also note that \\(Z\\) can take the range of values bounded by the interval \\((0,1)\\) (\\(Z \\rightarrow 0\\) as \\(X_2 \\rightarrow 0\\) and \\(Z \\rightarrow 1\\) as \\(X_2 \\rightarrow \\infty\\), since \\(Y \\rightarrow \\infty\\) too). Replacing \\(X_2\\) with \\(Z\\) in (8), we get:\n\\[\\begin{align}\rf_{Y}(y) \u0026amp; = \\int_{0}^{1} \\frac{1}{\\Gamma(\\alpha_1)} \\frac{1}{\\Gamma(\\alpha_2)} (y- yz)^{(\\alpha_1 -1)} (yz)^{(\\alpha_2 -1)} e^{-y} ydZ \\nonumber \\\\\r\\implies f_{Y}(y) \u0026amp; = \\int_{0}^{1} \\frac{1}{\\Gamma(\\alpha_1)} \\frac{1}{\\Gamma(\\alpha_2)} y^{(\\alpha_1 -1)} (1- z)^{(\\alpha_1 -1)} y^{(\\alpha_2 -1)} z^{(\\alpha_2 -1)} e^{-y} ydZ \\nonumber \\\\\r\\implies f_{Y}(y) \u0026amp; = \\int_{0}^{1} \\frac{1}{\\Gamma(\\alpha_1)} \\frac{1}{\\Gamma(\\alpha_2)} y^{(\\alpha_1 + \\alpha_2 -1)} e^{-y} (1- z)^{(\\alpha_1 -1)} z^{(\\alpha_2 -1)} dZ \\nonumber \\\\\r\\implies f_{Y}(y) \u0026amp; = \\frac{1}{\\Gamma(\\alpha_1)} \\frac{1}{\\Gamma(\\alpha_2)} y^{(\\alpha_1 + \\alpha_2 -1)} e^{-y} \\int_{0}^{1} (1- z)^{(\\alpha_1 -1)} z^{(\\alpha_2 -1)} dZ \\tag{9} \\end{align}\\]\nThe integral in (9) is the Beta function, whose value is given below:\n\\[\\begin{align}\r\\int_{0}^{1} (1- z)^{(\\alpha_1 -1)} z^{(\\alpha_2 -1)} dZ = \\frac{\\Gamma(\\alpha_1)\\Gamma(\\alpha_2)}{\\Gamma(\\alpha_1 + \\alpha_2)} \\tag{10}\r\\end{align}\\]\nSubstituting (10) back into (9), we get:\r\\[\\begin{align}\rf_{Y}(y) \u0026amp; = \\frac{1}{\\Gamma(\\alpha_1)} \\frac{1}{\\Gamma(\\alpha_2)} y^{(\\alpha_1 + \\alpha_2 -1)} e^{-y} \\frac{\\Gamma(\\alpha_1)\\Gamma(\\alpha_2)}{\\Gamma(\\alpha_1 + \\alpha_2)}\\nonumber \\\\\r\\implies f_{Y}(y) \u0026amp; = \\frac{1}{\\Gamma(\\alpha_1 + \\alpha_2)} y^{(\\alpha_1 + \\alpha_2 -1)} e^{-y} \\tag{11} \\\\\r\\implies f_{Y}(y) \u0026amp; = Gamma(\\alpha_1 + \\alpha_2, 1) \\nonumber\r\\end{align}\\]\nFrom (11), we have now proved that the property in (1) holds for \\(N=2\\). We now assume this is true for \\(N\\) and show that this property holds for \\(N+1\\). Now, let \\(Y = \\sum_{i=1}^{N} X_i\\), which gives us the following identities:\n\\[\\begin{align}\r\\sum_{i=1}^{N+1} X_i \u0026amp; = Y + X_{N+1} \\tag{12} \\\\\rY \u0026amp; \\sim Gamma(\\sum_{i=1}^{N} \\alpha_i,1) \\tag{13}\r\\end{align}\\]\n(13) follows from the induction assumption. Since the property in (1) is true for \\(N=2\\), we combine this with the identities in (12) \u0026amp; (13) to get:\n\\[\\begin{align}\r\\sum_{i=1}^{N+1} X_i \u0026amp; = Y + X_{N+1} \\sim Gamma(\\sum_{i=1}^{N} \\alpha_i + \\alpha_{N+1},1) \\nonumber \\\\\r\\implies \\sum_{i=1}^{N+1} X_i \u0026amp; \\sim Gamma(\\sum_{i=1}^{N+1} \\alpha_i,1) \\tag{14} \\end{align}\\]\n(14) shows that the property in (1) is true for \\(N+1\\), completing the proof.\nReferences\rAntoniak, Charles E. 1974. “Mixtures of Dirichlet Processes with Applications to Bayesian Nonparametric Problems.” The Annals of Statistics, 1152–74.\n\rFerguson, Thomas S. 1973. “A Bayesian Analysis of Some Nonparametric Problems.” The Annals of Statistics, 209–30.\n\r\r\r","date":1567468800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567558547,"objectID":"f0081b01add0652d81ba1d39b4bb0b10","permalink":"/post/additive-property-of-the-gamma-distribution/","publishdate":"2019-09-03T00:00:00Z","relpermalink":"/post/additive-property-of-the-gamma-distribution/","section":"post","summary":"While learning Bayesian Nonparametric methodology, I found a few properties useful in understanding the way Dirichlet Process Priors worked (this is a prior used very commonly in the field (Ferguson (1973), Antoniak (1974))) . This post will be one in a series that describes each of the aforementioned properties and provides simple proofs/examples of the same.\nAs the title suggests, we start with the Gamma distribution. We use the shape and rate parameterization of the distribution (as explained here).","tags":["Bayesian Nonparametrics","Gamma Distribution"],"title":"Additive Property of the Gamma Distribution","type":"post"},{"authors":[],"categories":["Scientific Computing"],"content":" I recently read a paper (Heiss and Winschel 2008) that advocated the use of certain techniques (Sparse Grids, SG henceforth) in numerical integration to calculate likelihood functions, as opposed to using Monte Carlo (MC henceforth) methods for the same. While approximating integrals with MC methods are simpler to implement, they might lead to integral values with considerable simulation error (Skrainka and Judd 2011). This post attempts to demonstrate the claim in Skrainka and Judd (2011) using two very simple integrals, to which we already know the value. I attempt to compare the outcomes from using MC and SG.\nThe integrals I’ll be evaluating are:\n\\[\\begin{equation} \\int_{-\\infty}^{\\infty} \\left( \\sum_{i=1}^{5} x_i \\right) dX \\tag{1} \\end{equation}\\] and \\[\\begin{equation} \\int_{-\\infty}^{\\infty} \\left( \\prod_{i=1}^{5} x_i^2 \\right) dX \\tag{2} \\end{equation}\\] where \\(X = \\{x_i\\}_{i=1}^{5}\\) is a five dimensional random variable, which is distributed according to the multivariate standard normal: \\[ X \\sim N\\left( \\left[ \\begin{array} {r} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ \\end{array}\\right], \\left[ \\begin{array} {rrrrr} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\\\ \\end{array}\\right] \\right) \\]\nGiven the distribution of \\(X\\), the values of the integrals above are easily obtained from standard results (the value of (1) is \\(0\\) and that of (2) is \\(1\\)) respectively.\nI write some utility functions in R to compute the integrands above:\n#function to compute the sum of components of the random vector s \u0026lt;- function(x) { return(sum(x)) } #function to compute the square product of the components of the random vector p \u0026lt;- function(x) { return(prod(x^2)) } I now write a function that:\n Simulates a certain number of draws from the distribution of the random variable \\(X\\) Computes the integrand function using each of these draws as input Takes the average of the values computed in the previous step  This function, in effect, would give us the approximate value of the integral via MC methodology.\nThe code is provided below, note that I use the mvtnorm package to create random draws.\nlibrary(mvtnorm) #Function to calculate the MC approximation for the integral mc_int \u0026lt;- function(s, n, mu, sigma) { #generate random draws x \u0026lt;- rmvnorm(n, mean = mu, sigma = sigma) #now get the integral mc_int_n \u0026lt;- mean(apply(x, 1, s)) return(mc_int_n) } set.seed(100) n \u0026lt;- 1000 mc_val \u0026lt;- mc_int(s, n, mu = rep(0,5), sigma = diag(5)) mc_val ## [1] 0.007150433 The result, 0.00715 is not far off from the true value of \\(0\\) at first glance, however, we need to compare this to the result from the SG approach.\nR has a package that generates sparse grids for numerical integration as described in Heiss and Winschel (2008), called SparseGrid. We now use the nodes and weights generated from this package to approximate the first integral. I re-use some of the code provided in the documentation for the SparseGrid package in R.\nlibrary(SparseGrid) #generate sparse grids for a 5 dimensional RV with accuracy level 2 sg \u0026lt;- createSparseGrid(type=\u0026#39;KPN\u0026#39;, dimension=5, k=2) sg_int \u0026lt;- function(func, sg, ...) { gx \u0026lt;- apply(sg$nodes, 1, function(x) {func(x, ...)}) return(sum(gx * sg$weights)) } sg_val \u0026lt;- sg_int(s, sg) sg_val ## [1] 0 The result here is exactly 0. In light of this finding, the value obtained from the MC approach, in comparison, is a little off, and tends to show a high variance in output:\nset.seed(100) mc_int(s, n, mu = rep(0,5), sigma = diag(5)) ## [1] 0.007150433 mc_int(s, n, mu = rep(0,5), sigma = diag(5)) ## [1] 0.03162326 mc_int(s, n, mu = rep(0,5), sigma = diag(5)) ## [1] -0.1287932 mc_int(s, n, mu = rep(0,5), sigma = diag(5)) ## [1] 0.01040134 mc_int(s, n, mu = rep(0,5), sigma = diag(5)) ## [1] -0.03798655 In the third case, there is a \\(-12\\%\\) error(!) in the value of the computed integral when compared to the result from the SG approach. The SG approach, in addition, shows no such variation in repeated runs, since the grid values and weights are fixed for a given accuracy level and dimension (of the variable being integrated).\nI repeat the calculations for the second integral, as shown below\nMC approach:\nset.seed(100) n \u0026lt;- 1000 mc_val \u0026lt;- mc_int(p, n, mu = rep(0,5), sigma = diag(5)) mc_val ## [1] 1.001089 SG approach:\n#generate sparse grids for a 5 dimensional RV with accuracy level 6 sg \u0026lt;- createSparseGrid(type=\u0026#39;KPN\u0026#39;, dimension=5, k=6) sg_val \u0026lt;- sg_int(p, sg) sg_val ## [1] 1 Once again, the SG approach gives us an exact value (note that the value of \\(k\\), the accuracy level, has gone up, since the integrand is a higher order function). Again, the difference of the results between the two approaches doesn’t seem that large. However, variability of the results from the MC approach is still a concern, as shown below:\nset.seed(100) mc_int(p, n, mu = rep(0,5), sigma = diag(5)) ## [1] 1.001089 mc_int(p, n, mu = rep(0,5), sigma = diag(5)) ## [1] 0.4672555 mc_int(p, n, mu = rep(0,5), sigma = diag(5)) ## [1] 1.14692 mc_int(p, n, mu = rep(0,5), sigma = diag(5)) ## [1] 1.062975 mc_int(p, n, mu = rep(0,5), sigma = diag(5)) ## [1] 0.9112416 In the second case, there is a roughly \\(53\\%\\) (!!) error when compared to the true value of the integral. This variability could be worse with more complicated integrands.\nOne suggestion to reduce variability in MC methods is to increase the number of draws, but that would entail a lot of calculations and result in longer runtimes.\nReferences Genz, Alan, Frank Bretz, Tetsuhisa Miwa, Xuefei Mi, Friedrich Leisch, Fabian Scheipl, and Torsten Hothorn. 2019. mvtnorm: Multivariate Normal and T Distributions. https://CRAN.R-project.org/package=mvtnorm.\n Heiss, Florian, and Viktor Winschel. 2008. “Likelihood Approximation by Numerical Integration on Sparse Grids.” Journal of Econometrics 144 (1). Elsevier: 62–80.\n Skrainka, Benjamin S, and Kenneth L Judd. 2011. “High Performance Quadrature Rules: How Numerical Integration Affects a Popular Model of Product Differentiation.” Available at SSRN 1870703.\n   ","date":1564272000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564620683,"objectID":"b808947d31eeb351808114760100dc6d","permalink":"/post/numerical-integration-with-sparse-grids/","publishdate":"2019-07-28T00:00:00Z","relpermalink":"/post/numerical-integration-with-sparse-grids/","section":"post","summary":"I recently read a paper (Heiss and Winschel 2008) that advocated the use of certain techniques (Sparse Grids, SG henceforth) in numerical integration to calculate likelihood functions, as opposed to using Monte Carlo (MC henceforth) methods for the same. While approximating integrals with MC methods are simpler to implement, they might lead to integral values with considerable simulation error (Skrainka and Judd 2011). This post attempts to demonstrate the claim in Skrainka and Judd (2011) using two very simple integrals, to which we already know the value.","tags":["Numerical Integration","Sparse Grids"],"title":"Numerical Integration With Sparse Grids","type":"post"}]