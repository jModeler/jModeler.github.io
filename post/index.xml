<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Journeyman Modeler</title>
    <link>/post/</link>
    <description>Recent content in Posts on Journeyman Modeler</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 25 Apr 2021 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Dirichlet Distribution Draws Function with Rcpp/RcppArmadillo</title>
      <link>/post/2021-04-25-dirichlet-distribution-draws-function-with-rcpp-rcpparmadillo/</link>
      <pubDate>Sun, 25 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/2021-04-25-dirichlet-distribution-draws-function-with-rcpp-rcpparmadillo/</guid>
      <description>
&lt;script src=&#34;../rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;This post constructs draws from a Dirichlet distribution from a gamma distribution with the help of Rcpp/RcppArmadillo. I’m writing this function to practice using C++ functions as opposed to their R counterparts when I see bottlenecks in my code. As explained in a &lt;a href=&#34;https://jmodeler.github.io/post/deriving-the-dirichlet-distribution-from-gamma-distributed-variables/&#34;&gt;previous post&lt;/a&gt;, the strategy to generate random draws from the Dirichlet distribution is as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Generate data from a gamma distribution, with the shape parameter given by the concentration parameter of the Dirichlet distribution&lt;/li&gt;
&lt;li&gt;Once we have all the gamma draws, normalize appropriately to get the Dirichlet distribution draws&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The code for this, using Rcpp and RcppArmadillo, is given below:&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;#include &amp;lt;RcppArmadillo.h&amp;gt;
// [[Rcpp::depends(RcppArmadillo)]]
using namespace Rcpp;

// [[Rcpp::export]]
arma::mat rdirichlet_cpp(int n, arma::vec concentration_parameters) {
  // get n random draws from the dirichlet distribution
  // n: number of draws needed
  // concentration_parameters: self explanatory, needed parameters for the dirichlet
  // distribution
  // create a variable to store the length of the concentration_parameters vector
  int k = concentration_parameters.n_elem;
  // next, create a matrix that will store the gamma draws
  // each row in this matrix will be a draw from the gamma distribution
  arma::mat gamma_draws(n, k, arma::fill::zeros);
  // create a matrix that stores the dirichlet distribution draws
  arma::mat dirichlet_draws(n, k, arma::fill::zeros);
  // create temporary variables that will store values in the loops
  // vector to store temporary gamma draws
  NumericVector temp_gamma(n);
  // variable to store the column sums of the gamma_draws matrix
  arma::vec temp_sum;
  // now, loop across the concentration parameters
  // get gamma draws and fill up the gamma_draws matrix
  for(int ii = 0; ii &amp;lt; k; ii++) {
    // get draws from gamma distribution, store in an armadillo vector
    temp_gamma = rgamma(n, concentration_parameters.at(ii));
    // temp_gamma is an Rcpp NumericVector, which needs to be converted into an armadillo vector and then
    // stored in the gamma_draws matrix
    gamma_draws.col(ii) = as&amp;lt;arma::vec&amp;gt;(temp_gamma);
  }
  // now normalize the rows to get the dirichlet draws
  // element-wise normalization using the each_col() method for a matrix object in armadillo
  // create the vector of row sums
  temp_sum = arma::sum(gamma_draws, 1);
  // now divide each column of gamma_draws with the elements in temp_sum
  dirichlet_draws = gamma_draws.each_col() / temp_sum;
  // return this final value
  return dirichlet_draws;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s check to see if the mean vector of the draws obtained from this function are in-line with expectations:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Rcpp)
library(RcppArmadillo)
# set number of data points needed
n &amp;lt;- 100000
# set dimension of data
k &amp;lt;- 3
# generate the concentration parameters 
alpha &amp;lt;- sample(1:100, k)
# calculate the actual mean vector
actual_mean = alpha/sum(alpha)
# get Dirichlet draws from the C++ function
dir_draws &amp;lt;- rdirichlet_cpp(n, alpha)
# get the column means
calculated_mean &amp;lt;- colMeans(dir_draws)
# compare both, should be close
(actual_mean - calculated_mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -2.310840e-05 -1.599541e-05  3.910381e-05&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The calculated mean is close to the actual mean. In a series of future posts, I will explain this function in detail and compare the speed of this function to other similar functions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dirichlet Distribution Draws Function with Rcpp/RcppArmadillo</title>
      <link>/post/2021-04-25-dirichlet-distribution-draws-function-with-rcpp-rcpparmadillo/</link>
      <pubDate>Sun, 25 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/2021-04-25-dirichlet-distribution-draws-function-with-rcpp-rcpparmadillo/</guid>
      <description>
&lt;script src=&#34;../post/2021-04-25-dirichlet-distribution-draws-function-with-rcpp-rcpparmadillo/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;This post constructs draws from a Dirichlet distribution from a gamma distribution with the help of Rcpp/RcppArmadillo. I’m writing this function to practice using C++ functions as opposed to their R counterparts when I see bottlenecks in my code. As explained in a &lt;a href=&#34;https://jmodeler.github.io/post/deriving-the-dirichlet-distribution-from-gamma-distributed-variables/&#34;&gt;previous post&lt;/a&gt;, the strategy to generate random draws from the Dirichlet distribution is as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Generate data from a gamma distribution, with the shape parameter given by the concentration parameter of the Dirichlet distribution&lt;/li&gt;
&lt;li&gt;Once we have all the gamma draws, normalize appropriately to get the Dirichlet distribution draws&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The code for this, using Rcpp and RcppArmadillo, is given below:&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;#include &amp;lt;RcppArmadillo.h&amp;gt;
// [[Rcpp::depends(RcppArmadillo)]]
using namespace Rcpp;

// [[Rcpp::export]]
arma::mat rdirichlet_cpp(int n, arma::vec concentration_parameters) {
  // get n random draws from the dirichlet distribution
  // n: number of draws needed
  // concentration_parameters: self explanatory, needed parameters for the dirichlet
  // distribution
  // create a variable to store the length of the concentration_parameters vector
  int k = concentration_parameters.n_elem;
  // next, create a matrix that will store the gamma draws
  // each row in this matrix will be a draw from the gamma distribution
  arma::mat gamma_draws(n, k, arma::fill::zeros);
  // create a matrix that stores the dirichlet distribution draws
  arma::mat dirichlet_draws(n, k, arma::fill::zeros);
  // create temporary variables that will store values in the loops
  // vector to store temporary gamma draws
  NumericVector temp_gamma(n);
  // variable to store the column sums of the gamma_draws matrix
  arma::vec temp_sum;
  // now, loop across the concentration parameters
  // get gamma draws and fill up the gamma_draws matrix
  for(int ii = 0; ii &amp;lt; k; ii++) {
    // get draws from gamma distribution, store in an armadillo vector
    temp_gamma = rgamma(n, concentration_parameters.at(ii));
    // temp_gamma is an Rcpp NumericVector, which needs to be converted into an armadillo vector and then
    // stored in the gamma_draws matrix
    gamma_draws.col(ii) = as&amp;lt;arma::vec&amp;gt;(temp_gamma);
  }
  // now normalize the rows to get the dirichlet draws
  // element-wise normalization using the each_col() method for a matrix object in armadillo
  // create the vector of row sums
  temp_sum = arma::sum(gamma_draws, 1);
  // now divide each column of gamma_draws with the elements in temp_sum
  dirichlet_draws = gamma_draws.each_col() / temp_sum;
  // return this final value
  return dirichlet_draws;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s check to see if the mean vector of the draws obtained from this function are in-line with expectations:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Rcpp)
library(RcppArmadillo)
# set number of data points needed
n &amp;lt;- 100000
# set dimension of data
k &amp;lt;- 3
# generate the concentration parameters 
alpha &amp;lt;- sample(1:100, k)
# calculate the actual mean vector
actual_mean = alpha/sum(alpha)
# get Dirichlet draws from the C++ function
dir_draws &amp;lt;- rdirichlet_cpp(n, alpha)
# get the column means
calculated_mean &amp;lt;- colMeans(dir_draws)
# compare both, should be close
(actual_mean - calculated_mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  1.802505e-04 -1.584761e-04 -2.177442e-05&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The calculated mean is close to the actual mean. In a series of future posts, I will explain this function in detail and compare the speed of this function to other similar functions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Useful Property of the Dirichlet Distribution</title>
      <link>/post/a-useful-property-of-the-dirichlet-distribution/</link>
      <pubDate>Sun, 07 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/a-useful-property-of-the-dirichlet-distribution/</guid>
      <description>


&lt;p&gt;This post is a continuation of the post that constructs a &lt;a href=&#34;https://jmodeler.github.io/post/deriving-the-dirichlet-distribution-from-gamma-distributed-variables/&#34;&gt;Dirichlet distribution from Gamma distributed variables&lt;/a&gt;. We use the same parameterization of the Gamma Distribution as before, and set the rate parameter &lt;span class=&#34;math inline&#34;&gt;\(=1\)&lt;/span&gt; unless otherwise specified. In this post I prove that the following property holds:&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(U = (U_1,\dots,U_{i},U_{i+1}, \dots, U_{N}) \sim Dir(\alpha_1,\dots,\alpha_i,\alpha_{i+1},\dots, \alpha_N)\)&lt;/span&gt; then &lt;span class=&#34;math inline&#34;&gt;\(U&amp;#39; = (U_1,U_2,\dots,U_{i}+U_{i+1}, \dots, U_{N}) \sim Dir(\alpha_1,\alpha_2,\dots,\alpha_i+\alpha_{i+1},\dots, \alpha_N)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In general, &lt;span class=&#34;math inline&#34;&gt;\(U&amp;#39;&amp;#39; = (\sum\limits_{i=1}^{k_1} U_i, \sum\limits_{i=k_1+1}^{k_2} U_i, \dots, \sum\limits_{i=k_j+1}^{N} U_i ) \sim Dir(\sum\limits_{i=1}^{k_1} \alpha_i, \sum\limits_{i=k_1+1}^{k_2} \alpha_i, \dots, \sum\limits_{i=k_j+1}^{N} \alpha_i )\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;I prove the first part of this property here. The rest follows by simply extending the proof. The strategy here is to construct IID Gamma distributed variables with appropriate parameters, and then derive the distribution of &lt;span class=&#34;math inline&#34;&gt;\(U&amp;#39;\)&lt;/span&gt; using the results from the &lt;a href=&#34;https://jmodeler.github.io/post/deriving-the-dirichlet-distribution-from-gamma-distributed-variables/&#34;&gt;previous post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Assume &lt;span class=&#34;math inline&#34;&gt;\(Z_i\)&lt;/span&gt;’s are IID &lt;span class=&#34;math inline&#34;&gt;\(\sim Gamma(\alpha_i,1)\)&lt;/span&gt;. From the &lt;a href=&#34;https://jmodeler.github.io/post/additive-property-of-the-gamma-distribution/&#34;&gt;additive property of the Gamma distribution&lt;/a&gt;, I have &lt;span class=&#34;math inline&#34;&gt;\(Z_{i,i+1} = Z_i + Z_{i+1} \sim Gamma(\alpha_i+\alpha_{i+1},1)\)&lt;/span&gt;. By definition, &lt;span class=&#34;math inline&#34;&gt;\(Z_{i,i+1}\)&lt;/span&gt; is independent of &lt;span class=&#34;math inline&#34;&gt;\(Z_j\)&lt;/span&gt;’s for &lt;span class=&#34;math inline&#34;&gt;\(1 \le j \le N, j \neq i, i+1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;I now define &lt;span class=&#34;math inline&#34;&gt;\(U&amp;#39; = (U_1,U_2,\dots,U_{i}+U_{i+1}, \dots, U_{N}) = (U_1,U_2,\dots,U_{i,i+1}, \dots, U_{N})\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(U_j = \frac{Z_j}{\sum\limits_{j\neq i, i+1}^{N} Z_j + Z_{i,i+1}}\)&lt;/span&gt;. From &lt;a href=&#34;https://jmodeler.github.io/post/deriving-the-dirichlet-distribution-from-gamma-distributed-variables/&#34;&gt;here&lt;/a&gt;, it follows that &lt;span class=&#34;math inline&#34;&gt;\(U&amp;#39; \sim Dir(\alpha_1,\alpha_2,\dots,\alpha_i+\alpha_{i+1},\dots, \alpha_N)\)&lt;/span&gt;. The second part of theorem  follows from similar reasoning.&lt;/p&gt;
&lt;p&gt;The result above can be used to get the marginal distributions of the &lt;span class=&#34;math inline&#34;&gt;\(U_i\)&lt;/span&gt;’s, where &lt;span class=&#34;math inline&#34;&gt;\(U = (U_1,\dots,U_i,\dots,U_N)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Define &lt;span class=&#34;math inline&#34;&gt;\(U&amp;#39; = (U_i, \sum\limits_{j \neq i}^{N} U_j)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;From the result above, this is distributed &lt;span class=&#34;math inline&#34;&gt;\(Dir(\alpha_i, \sum\limits_{j \neq i}^{N} \alpha_j) = Beta(\alpha_i, \sum\limits_{j \neq i}^{N} \alpha_j)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Since &lt;span class=&#34;math inline&#34;&gt;\(\sum\limits_{j \neq i}^{N} U_j = 1 - U_i\)&lt;/span&gt;, the density function can be expressed purely as a function of &lt;span class=&#34;math inline&#34;&gt;\(U_i\)&lt;/span&gt;, which is the marginal distribution of &lt;span class=&#34;math inline&#34;&gt;\(U_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Formally:&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(U = (U_1, \dots, U_{N}) \sim Dir(\alpha_1,\dots, \alpha_N)\)&lt;/span&gt; then &lt;span class=&#34;math inline&#34;&gt;\(U_i \sim Beta(\alpha_i, \sum\limits_{j \neq i}^{N} \alpha_j)\)&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;the-complete-neutral-property-of-the-dirichlet-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Complete Neutral Property of the Dirichlet Distribution&lt;/h2&gt;
&lt;p&gt;A somewhat related property is also discussed here:&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(U = (U_1, \dots, U_{N}) \sim Dir(\alpha_1,\dots, \alpha_N)\)&lt;/span&gt; and when &lt;span class=&#34;math inline&#34;&gt;\(k &amp;lt; N\)&lt;/span&gt;, then&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{(1-\sum\limits_{j = k+1}^{N} U_j)}(U_1,\dots,U_k) \sim Dir(\alpha_1,\dots,\alpha_k)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here, we note the term that divides each component of the vector &lt;span class=&#34;math inline&#34;&gt;\((U_1,\dots,U_k)\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\((1-\sum\limits_{j = k+1}^{N} U_j)\)&lt;/span&gt;, which is nothing but &lt;span class=&#34;math inline&#34;&gt;\(\sum\limits_{j = 1}^{k} U_j\)&lt;/span&gt;, since &lt;span class=&#34;math inline&#34;&gt;\(\sum\limits_{j = 1}^{N} U_j = 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Assume &lt;span class=&#34;math inline&#34;&gt;\(\{Z_i\}_{i=1}^{N}\)&lt;/span&gt;, are &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; IID Gamma distributed random variables, with &lt;span class=&#34;math inline&#34;&gt;\(Z_i \sim Gamma(\alpha_i,1) \, \forall \, i=1,\dots,N\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In the post on &lt;a href=&#34;https://jmodeler.github.io/post/deriving-the-dirichlet-distribution-from-gamma-distributed-variables/&#34;&gt;constructing a dirichlet distribution from Gamma distributed variables&lt;/a&gt;, it was shown that when &lt;span class=&#34;math inline&#34;&gt;\(U_i = Z_i/(\sum\limits_{j = 1}^{N} Z_j) \, \forall \, i=1,\dots,N\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(U = (U_1, \dots, U_{N}) \sim Dir(\alpha_1,\dots, \alpha_N)\)&lt;/span&gt;. With this construction, I now have:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:gamconstr1&#34;&gt;\[\begin{align}
X_i =  \frac{U_i}{1-\sum\limits_{j = k+1}^{N} U_j} = \frac{U_i}{\sum\limits_{j = 1}^{k} U_j} \, \forall \, i=1,\dots,k  \tag{1} 
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since &lt;span class=&#34;math inline&#34;&gt;\(U_i = Z_i/(\sum\limits_{j = 1}^{N} Z_j) \, \forall \, i=1,\dots,N\)&lt;/span&gt;, I put this in &lt;a href=&#34;#eq:gamconstr1&#34;&gt;(1)&lt;/a&gt; to get:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:gamconstr2&#34;&gt;\[\begin{align}
X_i = \frac{U_i}{\sum\limits_{j = 1}^{k} U_j} = \frac{Z_i}{\sum\limits_{j = 1}^{k} Z_j}  \, \forall \, i=1,\dots,k  \tag{2}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;From the post on &lt;a href=&#34;https://jmodeler.github.io/post/deriving-the-dirichlet-distribution-from-gamma-distributed-variables/&#34;&gt;constructing a dirichlet distribution from Gamma distributed variables&lt;/a&gt;, we know that &lt;span class=&#34;math inline&#34;&gt;\(X = (X_1,\dots, X_k) \sim Dir(\alpha_1,\dots,\alpha_k)\)&lt;/span&gt;, which completes the proof. This property of the Dirichlet Distribution is also called the &lt;span class=&#34;math inline&#34;&gt;\(\textit{complete neutral property}\)&lt;/span&gt; &lt;span class=&#34;citation&#34;&gt;(Albert and Denis &lt;a href=&#34;#ref-albert2011dirichlet&#34; role=&#34;doc-biblioref&#34;&gt;2011&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-albert2011dirichlet&#34;&gt;
&lt;p&gt;Albert, Isabelle, and Jean-Baptiste Denis. 2011. “Dirichlet and multinomial distributions: properties and uses in JAGS.” &lt;em&gt;Analysis&lt;/em&gt; 31: 1141–55.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Deriving the Dirichlet Distribution from Gamma Distributed Variables</title>
      <link>/post/deriving-the-dirichlet-distribution-from-gamma-distributed-variables/</link>
      <pubDate>Sun, 31 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/deriving-the-dirichlet-distribution-from-gamma-distributed-variables/</guid>
      <description>


&lt;p&gt;This post is a continuation of the post that constructs a &lt;a href=&#34;https://jmodeler.github.io/post/deriving-the-beta-distribution-from-gamma-distributed-variables/&#34;&gt;Beta distribution from Gamma distributed variables&lt;/a&gt;. We use the same parameterization of the Gamma Distribution as before, and set the rate parameter &lt;span class=&#34;math inline&#34;&gt;\(=1\)&lt;/span&gt; unless otherwise specified. In this post I prove that the following property holds:&lt;/p&gt;
&lt;p&gt;Given &lt;span class=&#34;math inline&#34;&gt;\(Z_i \sim Gamma(\alpha_i,1)\)&lt;/span&gt;, are IID, (&lt;span class=&#34;math inline&#34;&gt;\(\alpha_i &amp;gt; 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(1 \le i \le n\)&lt;/span&gt;) and I define the variables &lt;span class=&#34;math inline&#34;&gt;\(U_i = \frac{Z_i}{\sum\limits_{i=1}^{N} Z_i}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V = \sum\limits_{i=1}^{N} Z_i\)&lt;/span&gt;. Then:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((U_1, U_2, \dots, U_N) \sim Dir(\alpha_1, \alpha_2, \dots, \alpha_N) = \frac{\Gamma(\sum \alpha_i)}{\Gamma(\alpha_1)\dots \Gamma(\alpha_N)} u_1^{\alpha_1-1}u_2^{\alpha_1-1}\dots u_N^{\alpha_N-1} = \frac{\Gamma(\sum \alpha_i)}{\Gamma(\alpha_1)\dots \Gamma(\alpha_N)} u_1^{\alpha_1-1}u_2^{\alpha_1-1}\dots (1-\sum\limits_{i=1}^{N-1}u_i)^{\alpha_N-1}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(V \sim Gamma(\sum\limits_{i=1}^{N} \alpha_i, 1)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(U = (U_1, U_2, \dots, U_N)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; are independent&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By construction, &lt;span class=&#34;math inline&#34;&gt;\(\sum\limits_{i=1}^{N} U_i = 1\)&lt;/span&gt;. The &lt;span class=&#34;math inline&#34;&gt;\(N^{th}\)&lt;/span&gt; term is redundant (in the sense that it can be obtained from the remaining &lt;span class=&#34;math inline&#34;&gt;\((N-1)\)&lt;/span&gt; terms). I use the &lt;a href=&#34;http://andymiller.github.io/2015/08/09/integral-jacobian.html&#34;&gt;change of variables approach&lt;/a&gt; to find the joint distribution of &lt;span class=&#34;math inline&#34;&gt;\((U_1,U_2,\dots,U_{N-1}, V)\)&lt;/span&gt;. I represent the &lt;span class=&#34;math inline&#34;&gt;\(Z_i\)&lt;/span&gt;’s as functions of &lt;span class=&#34;math inline&#34;&gt;\(U_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; to get:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:zureln2&#34; id=&#34;eq:zureln1&#34;&gt;\[\begin{align}
Z_i &amp;amp; =   U_i V &amp;amp;&amp;amp; 0 \le i \le N-1  \tag{1}\\ 
Z_N &amp;amp; =  (1-\sum_{i=1}^{N-1}U_i)V &amp;amp;&amp;amp;  \tag{2}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;By definition, the joint density of &lt;span class=&#34;math inline&#34;&gt;\((U_1,U_2,\dots,U_{N-1}, V)\)&lt;/span&gt; is given by:
&lt;span class=&#34;math display&#34; id=&#34;eq:chovn&#34;&gt;\[\begin{align}
f_{U_1,U_2,\dots,U_{N-1}, V}(u_1,u_2,\dots, u_{N-1}, v) &amp;amp;= f_{Z_1,\dots,Z_N}(z_1,\dots,z_N) |J_{(z_1,z_2,\dots,z_{N-1},z_N) \rightarrow (u_1,u_2,\dots, u_{N-1}, v)}| &amp;amp; \tag{3}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The Jacobian matrix is given by:
&lt;span class=&#34;math display&#34; id=&#34;eq:njaco&#34;&gt;\[\begin{align}
J_{(z_1,z_2,\dots,z_{N-1},z_N) \rightarrow (u_1,u_2,\dots, u_{N-1}, v)} = 
\begin{bmatrix} 
\frac{\partial z_1}{\partial u_1} &amp;amp; \frac{\partial z_1}{\partial u_2} &amp;amp; \dots &amp;amp; \frac{\partial z_1}{\partial u_{N-1}} &amp;amp; \frac{\partial z_1}{\partial v} \\ 
\frac{\partial z_2}{\partial u_1} &amp;amp; \frac{\partial z_2}{\partial u_2} &amp;amp; \dots &amp;amp; \frac{\partial z_2}{\partial u_{N-1}} &amp;amp; \frac{\partial z_2}{\partial v} \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots \\
\frac{\partial z_{N-1}}{\partial u_1} &amp;amp; \frac{\partial z_{N-1}}{\partial u_2} &amp;amp; \dots &amp;amp; \frac{\partial z_{N-1}}{\partial u_{N-1}} &amp;amp; \frac{\partial z_{N-1}}{\partial v} \\
\frac{\partial z_N}{\partial u_1} &amp;amp; \frac{\partial z_N}{\partial u_2} &amp;amp; \dots &amp;amp; \frac{\partial z_N}{\partial u_N} &amp;amp; \frac{\partial z_N}{\partial v} 
\end{bmatrix} \tag{4}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Given &lt;a href=&#34;#eq:zureln1&#34;&gt;(1)&lt;/a&gt; &amp;amp; &lt;a href=&#34;#eq:zureln2&#34;&gt;(2)&lt;/a&gt;, the partial derivatives in the Jacobian matrix  are given by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:parzureln5&#34; id=&#34;eq:parzureln4&#34; id=&#34;eq:parzureln3&#34; id=&#34;eq:parzureln2&#34; id=&#34;eq:parzureln1&#34;&gt;\[\begin{alignat}{2}
\frac{\partial z_i}{\partial u_i} &amp;amp; =  v &amp;amp;&amp;amp; \quad 0 \le i \le N-1  \tag{5}\\
\frac{\partial z_i}{\partial u_j} &amp;amp; =  0 &amp;amp;&amp;amp; \quad j \neq i, 0 \le j \le N-1  \tag{6}\\
\frac{\partial z_i}{\partial v} &amp;amp; = u_i &amp;amp;&amp;amp; \quad 0 \le i \le N-1 \tag{7} \\
\frac{\partial z_N}{\partial u_i} &amp;amp; = -v &amp;amp;&amp;amp; \quad 0 \le i \le N-1 \tag{8} \\
\frac{\partial z_N}{\partial v} &amp;amp; = 1-\sum_{i=1}^{N-1} u_i  \tag{9}
\end{alignat}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Substituting &lt;a href=&#34;#eq:parzureln1&#34;&gt;(5)&lt;/a&gt;, &lt;a href=&#34;#eq:parzureln2&#34;&gt;(6)&lt;/a&gt;, &lt;a href=&#34;#eq:parzureln3&#34;&gt;(7)&lt;/a&gt;, &lt;a href=&#34;#eq:parzureln4&#34;&gt;(8)&lt;/a&gt; &amp;amp; &lt;a href=&#34;#eq:parzureln5&#34;&gt;(9)&lt;/a&gt; in &lt;a href=&#34;#eq:njaco&#34;&gt;(4)&lt;/a&gt;, we get
&lt;span class=&#34;math display&#34; id=&#34;eq:njaco1&#34;&gt;\[\begin{align}
J_{(z_1,z_2,\dots,z_{N-1},z_N) \rightarrow (u_1,u_2,\dots, u_{N-1}, v)} = 
\begin{bmatrix} 
v &amp;amp; 0 &amp;amp; \dots &amp;amp; 0 &amp;amp; u_1 \\ 
0 &amp;amp; v &amp;amp; \dots &amp;amp; 0 &amp;amp; u_2 \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \dots &amp;amp; v &amp;amp; u_{N-1} \\
-v &amp;amp; -v &amp;amp; \dots &amp;amp; -v &amp;amp; 1-\sum_{i=1}^{N-1} u_i  
\end{bmatrix} \tag{10}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;I need the determinant of the Jacobian matrix above. This can be computed easily after making a row transformation operation. Apply the transformation &lt;span class=&#34;math inline&#34;&gt;\(R_N \rightarrow R_N + \sum\limits_{i=1}^{N-1} R_i\)&lt;/span&gt; (where &lt;span class=&#34;math inline&#34;&gt;\(R_i\)&lt;/span&gt; is the &lt;span class=&#34;math inline&#34;&gt;\(i^{th}\)&lt;/span&gt; row of the determinant) to get:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:njacosimple&#34;&gt;\[\begin{align}
|J_{(z_1,z_2,\dots,z_{N-1},z_N) \rightarrow (u_1,u_2,\dots, u_{N-1}, v)}| = 
\begin{vmatrix} 
v &amp;amp; 0 &amp;amp; \dots &amp;amp; 0 &amp;amp; u_1 \\ 
0 &amp;amp; v &amp;amp; \dots &amp;amp; 0 &amp;amp; u_2 \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \dots &amp;amp; v &amp;amp; u_{N-1} \\
0 &amp;amp; 0 &amp;amp; \dots &amp;amp; 0 &amp;amp; 1  
\end{vmatrix} = v^{N-1}  \tag{11}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Using result &lt;a href=&#34;#eq:njacosimple&#34;&gt;(11)&lt;/a&gt; in &lt;a href=&#34;#eq:chovn&#34;&gt;(3)&lt;/a&gt;, I get:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:nchovint&#34;&gt;\[\begin{align}
f_{U_1,U_2,\dots,U_{N-1}, V}(u_1,u_2,\dots, u_{N-1}, v) = f_{Z_1,Z_2,\dots,Z_{N-1},Z_N}(z_1,z_2,\dots,z_{N-1},z_N) v^{N-1} \tag{12}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since &lt;span class=&#34;math inline&#34;&gt;\(Z_i\)&lt;/span&gt; are IID &lt;span class=&#34;math inline&#34;&gt;\(Gamma(\alpha_i,1)\)&lt;/span&gt; distributed:
&lt;span class=&#34;math display&#34; id=&#34;eq:ngamma&#34;&gt;\[\begin{align}
f_{Z_1,\dots,Z_N}(z_1,\dots,z_N) = \prod_{i=1}^{N}f_{Z_i}(z_i) = \prod_{i=1}^{N} \frac{1}{\Gamma(\alpha_i)} z_i^{\alpha_i - 1} e^{-z_i} = \left[ \prod_{i=1}^{N} \frac{1}{\Gamma(\alpha_i)} z_i^{\alpha_i - 1} \right] e^{-\sum\limits_{i=1}^{N}z_i} \tag{13}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Substituting &lt;a href=&#34;#eq:ngamma&#34;&gt;(13)&lt;/a&gt; into &lt;a href=&#34;#eq:nchovint&#34;&gt;(12)&lt;/a&gt;, and using the relations in &lt;a href=&#34;#eq:zureln1&#34;&gt;(1)&lt;/a&gt; &amp;amp; &lt;a href=&#34;#eq:zureln2&#34;&gt;(2)&lt;/a&gt;:
&lt;span class=&#34;math display&#34; id=&#34;eq:nchovint1&#34;&gt;\[\begin{align}
f_{U_1,\dots,U_{N-1}, V}(u_1,\dots, u_{N-1}, v)  =  &amp;amp; \left[ \prod_{i=1}^{N} \frac{1}{\Gamma(\alpha_i)} z_i^{\alpha_i - 1} \right] e^{-\sum\limits_{i=1}^{N}z_i} \cdot v^{N-1} &amp;amp; \nonumber \\
\implies f_{U_1,\dots,U_{N-1}, V}(u_1,\dots, u_{N-1}, v)  = &amp;amp; \left[ \prod_{i=1}^{N-1} \frac{1}{\Gamma(\alpha_i)} z_i^{\alpha_i - 1} \right] \frac{1}{\Gamma(\alpha_N)} z_N^{\alpha_N - 1} e^{-v}  v^{N-1} &amp;amp; \nonumber \\
\implies f_{U_1,\dots,U_{N-1}, V}(u_1,\dots, u_{N-1}, v)  = &amp;amp; \left[ \prod_{i=1}^{N-1} \frac{1}{\Gamma(\alpha_i)} (u_i v)^{\alpha_i - 1} \right] \frac{1}{\Gamma(\alpha_N)} (v(1-\sum\limits_{i=1}^{N-1}u_i))^{\alpha_N - 1} e^{-v}  v^{N-1} &amp;amp;  \tag{14}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since &lt;span class=&#34;math inline&#34;&gt;\(U_N = 1 - \sum\limits_{i=1}^{N-1}U_i\)&lt;/span&gt;, I put this in &lt;a href=&#34;#eq:nchovint1&#34;&gt;(14)&lt;/a&gt; to get
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
 f_{U_1,\dots,U_{N-1}, V}(u_1,\dots, u_{N-1}, v) &amp;amp; = \left[ \prod_{i=1}^{N-1} \frac{1}{\Gamma(\alpha_i)} (u_i v)^{\alpha_i - 1} \right] \frac{1}{\Gamma(\alpha_N)} (u_N v)^{\alpha_N - 1} e^{-v}  v^{N-1} &amp;amp; \nonumber \\
\implies f_{U_1,\dots,U_{N-1}, V}(u_1,\dots, u_{N-1}, v) &amp;amp; = \left[ \prod_{i=1}^{N} \frac{1}{\Gamma(\alpha_i)} (u_i v)^{\alpha_i - 1} \right] e^{-v}  v^{N-1} &amp;amp; \nonumber \\
\implies f_{U_1,\dots,U_{N-1}, V}(u_1,\dots, u_{N-1}, v) &amp;amp; = \left[ \prod_{i=1}^{N} \frac{1}{\Gamma(\alpha_i)} (u_i)^{\alpha_i - 1} \right] v^{\left(\sum\limits_{i=1}^{N}\alpha_i-N \right)} e^{-v}  v^{N-1} &amp;amp; \nonumber \\
\implies f_{U_1,\dots,U_{N-1}, V}(u_1,\dots, u_{N-1}, v) &amp;amp; = \left[ \prod_{i=1}^{N} \frac{1}{\Gamma(\alpha_i)} (u_i)^{\alpha_i - 1} \right] v^{\left(\sum\limits_{i=1}^{N}\alpha_i-1 \right)} e^{-v}  &amp;amp; \nonumber 
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
\implies f_{U_1,\dots,U_{N-1}, V}(u_1,\dots, u_{N-1}, v) &amp;amp; =  \Gamma \left(\sum\limits_{i=1}^{N}\alpha_i \right) \left[ \prod_{i=1}^{N} \frac{1}{\Gamma(\alpha_i)} (u_i)^{\alpha_i - 1} \right] \frac{1}{\Gamma \left(\sum\limits_{i=1}^{N}\alpha_i \right)}v^{\left(\sum\limits_{i=1}^{N}\alpha_i-1 \right)} e^{-v}  &amp;amp; \nonumber \\
\implies f_{U_1,\dots,U_{N-1}, V}(u_1,\dots, u_{N-1}, v) &amp;amp; =  \frac{\Gamma \left(\sum\limits_{i=1}^{N}\alpha_i \right)}{\prod\limits_{i=1}^{N} \Gamma(\alpha_i)} \left[ \prod_{i=1}^{N} (u_i)^{\alpha_i - 1} \right] \frac{1}{\Gamma \left(\sum\limits_{i=1}^{N}\alpha_i \right)}v^{\left(\sum\limits_{i=1}^{N}\alpha_i-1 \right)} e^{-v}  &amp;amp; \nonumber \\
\implies f_{U_1,\dots,U_{N-1}, V}(u_1,\dots, u_{N-1}, v) &amp;amp; =  Dir(\alpha_1,\alpha_2, \dots, \alpha_N) \cdot Gamma(\sum\limits_{i=1}^{N}\alpha_i, 1)  &amp;amp; 
\end{align}\]&lt;/span&gt;

Which establishes the following:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((U_1,U_2,\dots,U_{N-1}, U_{N}) \sim Dir(\alpha_1,\alpha_2, \dots, \alpha_N)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(V \sim Gamma(\sum\limits_{i=1}^{N}\alpha_i,1)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(U = (U_1,U_2,\dots,U_{N-1}, U_{N})\)&lt;/span&gt; &amp;amp; &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; are independent&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This completes the proof.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dirichlet Distribution from Gamma Distribution: Simple Examples</title>
      <link>/post/dirichlet-distribution-from-gamma-distribution-simple-examples/</link>
      <pubDate>Sun, 31 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/dirichlet-distribution-from-gamma-distribution-simple-examples/</guid>
      <description>


&lt;p&gt;This post is a continuation of the post that constructs a &lt;a href=&#34;https://jmodeler.github.io/post/deriving-the-dirichlet-distribution-from-gamma-distributed-variables/&#34;&gt;Dirichlet distribution from Gamma distributed variables&lt;/a&gt;. I now provide an empirical example of this construction for a simple case. In all of the work that follows, Unless otherwise specified, the rate parameter for the Gamma distribution is 1.&lt;/p&gt;
&lt;p&gt;I follow the steps below to do this:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Generate data from a gamma distribution&lt;/li&gt;
&lt;li&gt;Apply the appropriate transformation to these&lt;/li&gt;
&lt;li&gt;Check the distribution of the transformed variables&lt;/li&gt;
&lt;li&gt;Compare against data generated from a Dirichlet distribution with appropriate parameters&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;preliminaries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Preliminaries&lt;/h1&gt;
&lt;p&gt;I write a function that generates IID draws from &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; different gamma distributions, all with rate parameter = 1. Note that this is purely a utility function with no sanity checks on the input.
In the code below, the parameter &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is implicit, in that it is the length of the vector of shape parameters, &lt;span class=&#34;math inline&#34;&gt;\(alpha\)&lt;/span&gt;, which is being sent as an input (all the elements in the vector &lt;span class=&#34;math inline&#34;&gt;\(alpha\)&lt;/span&gt; must be &lt;span class=&#34;math inline&#34;&gt;\(&amp;gt;0\)&lt;/span&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Function to generate n random draws of dimension from d different gamma distributions

mv_gamma &amp;lt;- function(rgamma, n, alpha)
{
  # rgamma: the base r function to generate random draws from a univariate gamma distribution
  # n: number of draws needed
  # alpha: vector of shape parameters
  
  # generate random draws for each dimension, then put them together into an array
  # using sapply for this
  
  mvg_draws &amp;lt;- sapply(alpha, function(x) rgamma(n, x))
  
  #return this value
  return(mvg_draws)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I create another utility function to transform the output from the previous function to construct Dirichlet distributed variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Function to construct dirichlet distributed variables from gamma distributed variables

construct_dirichlet &amp;lt;- function(mvg_draws)
{
  # mvg_draws: output from the mv_gamma function. Typically an n x d matrix.  
  
  # Transform the columns of mvg_draws appropriately:
  # Divide each row with its sum
  
  const_dirichlet &amp;lt;- mvg_draws/rowSums(mvg_draws)
  
  #return this value
  return(const_dirichlet)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, a utility function to compare the distrbution of the constructed and actual random variables. Note that I use the &lt;span class=&#34;math inline&#34;&gt;\(\texttt{cramer.test()}\)&lt;/span&gt; function from the &lt;a href=&#34;https://cran.r-project.org/package=cramer&#34;&gt;cramer&lt;/a&gt; package to compare the multivariate draws.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;compare_distributions &amp;lt;- function(const_dirichlet, actual_dirichlet) {
  # const_dirichlet: n x d matrix containing the constructed dirichlet random draws
  # actual_dirichlet: n x d matrix containing the actual dirichlet random draws
  
  # get the cramer test result
  compare &amp;lt;- cramer.test(const_dirichlet, actual_dirichlet)
  
  # check the result of this hypothesis test
  if(compare$result==0) {
    cat(&amp;#39;Unable to reject hypothesis of equal distributions&amp;#39;)
  } else {
    cat(&amp;#39;Hypothesis of equal distributions should not be accepted&amp;#39;)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lastly, another utility function to compare (via plot) the distribution of the constructed and actual random variables. I use the &lt;span class=&#34;math inline&#34;&gt;\(\texttt{rdirichlet()}\)&lt;/span&gt; function from the &lt;a href=&#34;https://cran.r-project.org/package=cramer&#34;&gt;MCMCpack&lt;/a&gt; package to generate draws from the dirichlet distribution.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Function to plot densities of the constructed and actual dirichlet distribution

plot_compare &amp;lt;- function(const_dirichlet, actual_dirichlet, d)
{
  # const_dirichlet: n x d matrix containing the constructed dirichlet random draws
  # actual_dirichlet: n x d matrix containing the actual dirichlet random draws
  # d: the column id/component of the draw needed from both constucted and actual for comparison

  # Get the number of data points
  n &amp;lt;- nrow(const_dirichlet)
  
  # Get the draws in the dimension or column whose distribution need to be compared to the actual
  dcol &amp;lt;- const_dirichlet[,d]
  
  # Get the draws in the dimension or column, from the actual draws
  dcol_actual &amp;lt;- actual_dirichlet[,d]
  
  #create a data.frame of these values
  plot_data &amp;lt;- data.frame(draws = c(dcol, dcol_actual), type = rep(c(&amp;#39;constructed&amp;#39;, &amp;#39;actual&amp;#39;)), each = n)
  
  #plot this function
  ggplot(data = plot_data, aes(x = draws, color = type)) + geom_density() + xlab(&amp;#39;draws&amp;#39;) + ylab(&amp;#39;density&amp;#39;)
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;dimensional-case&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;3 Dimensional Case&lt;/h1&gt;
&lt;p&gt;Let’s use the functions we just created. I follow the steps outlined in the introduction of this post.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(cramer)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: boot&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;suppressPackageStartupMessages(library(MCMCpack))

# number of draws
n &amp;lt;- 1000

# multivariate shapre parameter
alpha &amp;lt;- c(1,2,3)

# get 3D IID gamma density draws
set.seed(100)
mvg_draws &amp;lt;- mv_gamma(rgamma, n, alpha)

# get the dirichlet draws from construction
const_dirichlet &amp;lt;- construct_dirichlet(mvg_draws)

# get the dirichlet draws from the actual distribution
set.seed(100)
actual_dirichlet &amp;lt;- rdirichlet(n, alpha)

# check if the draws come from the same underlying distribution
compare_distributions(const_dirichlet, actual_dirichlet)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Unable to reject hypothesis of equal distributions&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As can be seen above, the null hypothesis that the underlying distributions are the same cannot be rejected.
Let’s also look at the empirical densities of each of the columns of the constructed and actual dirichlet distribution draws.&lt;/p&gt;
&lt;p&gt;Dimension 1:
&lt;img src=&#34;../post/2020-05-31-dirichlet-distribution-from-gamma-distribution-simple-examples.en_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Dimension 2:
&lt;img src=&#34;../post/2020-05-31-dirichlet-distribution-from-gamma-distribution-simple-examples.en_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Dimension 3:
&lt;img src=&#34;../post/2020-05-31-dirichlet-distribution-from-gamma-distribution-simple-examples.en_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;These density plots look somewhat similar. When we increase the number of draws generated from both methods, these plots almost overlap:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# number of draws
n &amp;lt;- 100000

# multivariate shapre parameter
alpha &amp;lt;- c(1,2,3)

# get 3D IID gamma density draws
set.seed(100)
mvg_draws &amp;lt;- mv_gamma(rgamma, n, alpha)

# get the dirichlet draws from construction
const_dirichlet &amp;lt;- construct_dirichlet(mvg_draws)

# get the dirichlet draws from the actual distribution
set.seed(100)
actual_dirichlet &amp;lt;- rdirichlet(n, alpha)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Dimension 1:
&lt;img src=&#34;../post/2020-05-31-dirichlet-distribution-from-gamma-distribution-simple-examples.en_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Dimension 2:
&lt;img src=&#34;../post/2020-05-31-dirichlet-distribution-from-gamma-distribution-simple-examples.en_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Dimension 3:
&lt;img src=&#34;../post/2020-05-31-dirichlet-distribution-from-gamma-distribution-simple-examples.en_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Comparing the draws for different values of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; (i.e. different lengths of the &lt;span class=&#34;math inline&#34;&gt;\(alpha\)&lt;/span&gt; vector), is left as an exercise to the reader. The functions above can be re-used for different values of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;, but must be modified accordingly to include sanity checks for inappropriate input values and edge cases, via &lt;a href=&#34;http://r-pkgs.had.co.nz/tests.html&#34;&gt;unit testing&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Homotopy Principle: Simple Examples</title>
      <link>/post/the-homotopy-principle-simple-examples/</link>
      <pubDate>Sun, 26 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/the-homotopy-principle-simple-examples/</guid>
      <description>


&lt;p&gt;This post presents 2 simple examples of the homotopy principle applied to (fairly easy) linear and nonlinear systems of equations. At a very abstract level, given a system of equations for which a solution is needed, we convert this system to one whose solution we already know (or is easy to find out), and then bend this system till we get the solution to the original set of equations. These methods have very broad applications &lt;span class=&#34;citation&#34;&gt;(Garcia and Zangwill &lt;a href=&#34;#ref-garciapathways&#34; role=&#34;doc-biblioref&#34;&gt;1981&lt;/a&gt;)&lt;/span&gt;, and have been applied in the context of finding equlibria in Static Games &lt;span class=&#34;citation&#34;&gt;(Bajari et al. &lt;a href=&#34;#ref-Bajari2010&#34; role=&#34;doc-biblioref&#34;&gt;2010&lt;/a&gt;)&lt;/span&gt; and Dynamic Games &lt;span class=&#34;citation&#34;&gt;(Borkovsky, Doraszelski, and Kryukov &lt;a href=&#34;#ref-Borkovsky2010&#34; role=&#34;doc-biblioref&#34;&gt;2010&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;example-1-linear-system&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 1: Linear System&lt;/h2&gt;
&lt;p&gt;Say we want to find the solution to the following system of linear equations:
&lt;span class=&#34;math display&#34; id=&#34;eq:linEx1&#34;&gt;\[\begin{align}
\left[ \begin{array}
{rrrrr}
1 &amp;amp; 2   \\
3 &amp;amp; 4   \\
\end{array}\right] \left[ \begin{array} {r} 
x_1 \\
x_2 \\
\end{array} \right] = \left[ \begin{array} {l} 
5 \\
11 \\
\end{array} \right] \tag{1} 
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Readers should very easily be able to verify that the unique solution to this system is
&lt;span class=&#34;math display&#34;&gt;\[
(x_1,x_2) = (1,2)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let’s convert this system and introduce an additional parameter &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;, called the &lt;em&gt;homotopy parameter&lt;/em&gt;, which varies from &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;. Let’s call this new system &lt;span class=&#34;math inline&#34;&gt;\(H(x_1, x_2, t)\)&lt;/span&gt;
&lt;span class=&#34;math display&#34; id=&#34;eq:HFunc&#34;&gt;\[\begin{align}
\left[ \begin{array}
{rr}
1 &amp;amp; 2   \\
3 &amp;amp; 4   \\
\end{array}\right] \left[ \begin{array} {r} 
x_1 \\
x_2 \\
\end{array} \right] = \left[ \begin{array} {l} 
5t \\
11t \\
\end{array} \right] \tag{2} 
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;When &lt;span class=&#34;math inline&#34;&gt;\(t=0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(H(x_1, x_2, 0)\)&lt;/span&gt; yields the trivial (and only) solution &lt;span class=&#34;math inline&#34;&gt;\((x_1,x_2) = (0,0)\)&lt;/span&gt;. When &lt;span class=&#34;math inline&#34;&gt;\(t=1\)&lt;/span&gt;, we get our original system of equations back. When we solve for &lt;span class=&#34;math inline&#34;&gt;\((x_1, x_2)\)&lt;/span&gt; as a function of &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;, we get:
&lt;span class=&#34;math display&#34;&gt;\[
(x_1(t),x_2(t)) = (t,2t)
\]&lt;/span&gt;
At &lt;span class=&#34;math inline&#34;&gt;\(t=1\)&lt;/span&gt;, this will give us the solution we desire. Tracing the path of the solution gives us the following plots:
&lt;img src=&#34;../post/2020-04-26-the-homotopy-principle-simple-examples_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../post/2020-04-26-the-homotopy-principle-simple-examples_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../post/2020-04-26-the-homotopy-principle-simple-examples_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From this very simple example, we note that the general process followed is given below &lt;span class=&#34;citation&#34;&gt;(Garcia and Zangwill &lt;a href=&#34;#ref-garciapathways&#34; role=&#34;doc-biblioref&#34;&gt;1981&lt;/a&gt;)&lt;/span&gt;:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Convert the system of equations into one that has a known solution (i.e. &lt;span class=&#34;math inline&#34;&gt;\(H(x_1, x_2, 0)\)&lt;/span&gt; case above)&lt;/li&gt;
&lt;li&gt;Introduce a new parameter &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;, that gives the known system at &lt;span class=&#34;math inline&#34;&gt;\(t=0\)&lt;/span&gt; and the system for which the solutions are desired when &lt;span class=&#34;math inline&#34;&gt;\(t=1\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Trace the path of the solutions by changing the value of &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;example-2-nonlinear-system&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 2: Nonlinear System&lt;/h2&gt;
&lt;p&gt;Consider the following system of equations (from chapter 1, exercise 6 of &lt;span class=&#34;citation&#34;&gt;(Garcia and Zangwill &lt;a href=&#34;#ref-garciapathways&#34; role=&#34;doc-biblioref&#34;&gt;1981&lt;/a&gt;)&lt;/span&gt;):
&lt;span class=&#34;math display&#34; id=&#34;eq:nonlinEx1&#34;&gt;\[\begin{align}
F(x_1, x_2) = 
\left[ \begin{array}
{l}
e^{2 x_1} - x^{2}_{2} + 3   \\
4x_{2}e^{2 x_1} - x^{3}_{2}   \\
\end{array} \right]
 = \left[ \begin{array} {l} 
0 \\
0 \\
\end{array} \right] \tag{3} \\
(x_1, x_2) \in \mathbb{R}^2 \\
F: \mathbb{R}^2 \rightarrow \mathbb{R}^2
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Again, readers should verify that the solutions to this system of equations are &lt;span class=&#34;math inline&#34;&gt;\((x1, x2) = (0, -2) \, \&amp;amp; \, (x1, x2) = (0, 2)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We now introduce the homotopy parameter &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;, and define &lt;span class=&#34;math inline&#34;&gt;\(H(x_1, x_2, t)\)&lt;/span&gt; as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:HfuncNonlin1&#34;&gt;\[\begin{align}
H(x_1, x_2, t) = F(x_1, x_2) - (1-t)F(0, 0) \tag{4}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The formulation in &lt;a href=&#34;#eq:HfuncNonlin1&#34;&gt;(4)&lt;/a&gt; is called the &lt;em&gt;Newton Homotopy&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Garcia and Zangwill &lt;a href=&#34;#ref-garciapathways&#34; role=&#34;doc-biblioref&#34;&gt;1981&lt;/a&gt;)&lt;/span&gt;. A distinct advantage of this formulation, is that at &lt;span class=&#34;math inline&#34;&gt;\(t=0\)&lt;/span&gt;, it is easy to see that the solution to the system is &lt;span class=&#34;math inline&#34;&gt;\((0, 0)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For any &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; between &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(H(x_1, x_2, t)\)&lt;/span&gt; becomes:
&lt;span class=&#34;math display&#34; id=&#34;eq:HfuncNonlin2&#34;&gt;\[\begin{align}
\left[ \begin{array}
{l}
e^{2 x_1} - x^{2}_{2} + 4t - 1   \\
4x_{2}e^{2 x_1} - x^{3}_{2}   \\
\end{array} \right]
 = \left[ \begin{array} {l} 
0 \\
0 \\
\end{array} \right] 
\tag{5}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We now attempt to find &lt;span class=&#34;math inline&#34;&gt;\((x_1, x_2)\)&lt;/span&gt; as functions of &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. From &lt;a href=&#34;#eq:HfuncNonlin2&#34;&gt;(5)&lt;/a&gt;, we have:
&lt;span class=&#34;math display&#34; id=&#34;eq:x2Sol&#34;&gt;\[\begin{align}
  x_{2}^{3} = 4x_{2}e^{2 x_1} \nonumber \\
  x_2 = 0 \,\,\,\,\, OR \,\,\,\,\, x_{2} = \pm 2e^{x_1} \tag{6}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;When &lt;span class=&#34;math inline&#34;&gt;\(x_2 = 0\)&lt;/span&gt;, putting this back in &lt;a href=&#34;#eq:HfuncNonlin2&#34;&gt;(5)&lt;/a&gt; we get:
&lt;span class=&#34;math display&#34; id=&#34;eq:x1Sol1&#34;&gt;\[\begin{align}
  e^{2 x_1} + 4t - 1 = 0 \nonumber \\
  \implies x_1 = \frac{1}{2} log(1-4t) \tag{7} \\
  where \,\,\,\, 0 \le t \le 1/4
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;When &lt;span class=&#34;math inline&#34;&gt;\(x_2 = \pm 2e^{x_1}\)&lt;/span&gt;, putting this back in &lt;a href=&#34;#eq:HfuncNonlin2&#34;&gt;(5)&lt;/a&gt; we get:
&lt;span class=&#34;math display&#34; id=&#34;eq:x1Sol2&#34;&gt;\[\begin{align}
  -3e^{2 x_1} + 4t - 1 = 0 \nonumber \\
  \implies x_1 = \frac{1}{2} log\left(\frac{4t-1}{3}\right) \tag{8} \\
  where \,\,\,\, 1/4 &amp;lt; t \le 1
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Combining all the findings from &lt;a href=&#34;#eq:x2Sol&#34;&gt;(6)&lt;/a&gt;, &lt;a href=&#34;#eq:x1Sol1&#34;&gt;(7)&lt;/a&gt; and &lt;a href=&#34;#eq:x1Sol2&#34;&gt;(8)&lt;/a&gt;, we get:
&lt;span class=&#34;math display&#34; id=&#34;eq:x2t&#34; id=&#34;eq:x1t&#34;&gt;\[\begin{align}
  x_1(t) = \begin{cases}
        \frac{1}{2} log(1-4t) &amp;amp; \text{for } 0\le t \le 1/4\\
        \frac{1}{2} log\left(\frac{4t-1}{3}\right) &amp;amp; \text{for } 1/4 &amp;lt; t \leq 1
        \end{cases} \tag{9} \\
    x_1(t) = \begin{cases}
        0 &amp;amp; \text{for } 0\le t \le 1/4\\
        \pm 2 \sqrt{\left(\frac{4t-1}{3}\right)} &amp;amp; \text{for } 1/4 &amp;lt; t \leq 1
        \end{cases} \tag{10}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Which gives us the solution to the system of equations in &lt;a href=&#34;#eq:nonlinEx1&#34;&gt;(3)&lt;/a&gt; at &lt;span class=&#34;math inline&#34;&gt;\(t = 1\)&lt;/span&gt;. However, note that the functions &lt;span class=&#34;math inline&#34;&gt;\(x_1(t), x_2(t)\)&lt;/span&gt; are non-differentiable, which disqualifies them from being solution paths &lt;span class=&#34;citation&#34;&gt;(Garcia and Zangwill &lt;a href=&#34;#ref-garciapathways&#34; role=&#34;doc-biblioref&#34;&gt;1981&lt;/a&gt;)&lt;/span&gt;. This is evident in the plots shown below:
&lt;img src=&#34;../post/2020-04-26-the-homotopy-principle-simple-examples_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../post/2020-04-26-the-homotopy-principle-simple-examples_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;One could always try another formulation for &lt;span class=&#34;math inline&#34;&gt;\(H(x_1, x_2, t)\)&lt;/span&gt; which leads to well defined paths to the desired solution from the known solution (i.e. the solution to &lt;span class=&#34;math inline&#34;&gt;\(H(x_1, x_2, 0)\)&lt;/span&gt;). That is left as an exercise to the reader.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-Bajari2010&#34;&gt;
&lt;p&gt;Bajari, Patrick, Han Hong, John Krainer, and Denis Nekipelov. 2010. “Computing Equilibria in Static Games of Incomplete Information Using the All-Solution Homotopy.” &lt;em&gt;Operations Research&lt;/em&gt; 58 (4-part 2).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Borkovsky2010&#34;&gt;
&lt;p&gt;Borkovsky, Ron N., Ulrich Doraszelski, and Yaroslav Kryukov. 2010. “A user’s guide to solving dynamic stochastic games using the homotopy method.” &lt;em&gt;Operations Research&lt;/em&gt; 58 (4 PART 2): 1116–32. &lt;a href=&#34;https://doi.org/10.1287/opre.1100.0843&#34;&gt;https://doi.org/10.1287/opre.1100.0843&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-garciapathways&#34;&gt;
&lt;p&gt;Garcia, C B, and W I Zangwill. 1981. “Pathways to solutions, fixed points, and equilibria. 1981.” Prentice-Hall, Englewood Cliffs, NJ.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Deriving the Beta Distribution from Gamma Distributed Variables</title>
      <link>/post/deriving-the-beta-distribution-from-gamma-distributed-variables/</link>
      <pubDate>Sat, 07 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/deriving-the-beta-distribution-from-gamma-distributed-variables/</guid>
      <description>


&lt;p&gt;This post is a continuation of the post that proves the additive property of the &lt;a href=&#34;https://jmodeler.github.io/post/additive-property-of-the-gamma-distribution/&#34;&gt;Gamma distribution&lt;/a&gt;. We use the same parameterization of the Gamma Distribution as before, and set the rate parameter &lt;span class=&#34;math inline&#34;&gt;\(=1\)&lt;/span&gt; unless otherwise specified. In this post I prove that the following property holds:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:BetaProp&#34;&gt;\[\begin{align}
\frac{X_i}{\sum_{i=1}^{N} X_i} \sim Beta(\alpha_i, \sum_{j \ne i} \alpha_j)   \tag{1} \\
 Where \, X_i \sim Gamma(\alpha_i, 1), \, \, \{i,j\} \in \{1,2, \dots, N\}  
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:BetaDis&#34; id=&#34;eq:GammaDis&#34;&gt;\[\begin{align}
Gamma(\alpha_i, 1) = f_{X_i}(x; \alpha_i) = \frac{1}{\Gamma(\alpha_i)}  x^{(\alpha_i -1)} e^{-x}                 \tag{2} \\
Beta(\alpha_i, \sum_{j \ne i} \alpha_j) = f_{Y}(y; \{\alpha_i\}_{i=1,\dots,N}) = \frac{\Gamma(\sum_{i=1}^{N} \alpha_i)}{\Gamma(\alpha_i) \Gamma(\sum_{j \ne i} \alpha_j)} y^{\alpha_i - 1} (1-y)^{(\sum_{j \ne i} \alpha_j - 1)}
\tag{3}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Importantly, the &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt;’s are independent. Let’s attempt to prove this for a small value of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;. For &lt;span class=&#34;math inline&#34;&gt;\(N=1\)&lt;/span&gt;, the fraction in &lt;a href=&#34;#eq:BetaProp&#34;&gt;(1)&lt;/a&gt; is no longer random. For &lt;span class=&#34;math inline&#34;&gt;\(N=2\)&lt;/span&gt;, we derive the property in &lt;a href=&#34;#eq:BetaProp&#34;&gt;(1)&lt;/a&gt; from first principles. Let &lt;span class=&#34;math inline&#34;&gt;\(Y = \frac{X_1}{X_1 + X_2}\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt; are independent, Gamma distributed variables&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. We try to obtain the distribution of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:YdisInit&#34;&gt;\[\begin{align}
P(Y \le y)  &amp;amp; =  P(\frac{X_1}{X_1 + X_2} \le y)   \nonumber \\
\implies P(Y \le y)  &amp;amp; = P(X_1 \le y(X_1 + X_2))   \nonumber \\
\implies P(Y \le y)  &amp;amp; = P(X_1 (1-y) \le y X_2))   \nonumber \\
\implies P(Y \le y)  &amp;amp; = P(X_1  \le \frac{y}{1-y} X_2))     \tag{4} 
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Using similar logic from the &lt;a href=&#34;https://jmodeler.github.io/post/additive-property-of-the-gamma-distribution/&#34;&gt;previous post&lt;/a&gt;, we know that the expression in &lt;a href=&#34;#eq:YdisInit&#34;&gt;(4)&lt;/a&gt; is a double integral, one to vary &lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt; over the range of values it can possibly take: &lt;span class=&#34;math inline&#34;&gt;\((0, \infty)\)&lt;/span&gt;, and the other to vary &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\((0, \frac{y}{1-y} X_2)\)&lt;/span&gt;. This integral takes the form below:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:YInt&#34;&gt;\[\begin{align}
P(Y \le y) &amp;amp;  = P(X_1  \le \frac{y}{1-y} X_2)) = \int_{0}^{\infty} \left( \int_{0}^{\frac{y}{1-y} X_2}  \frac{1}{\Gamma(\alpha_1)}  x_{1}^{(\alpha_1 -1)} e^{-x_1} dX_1 \right)  \frac{1}{\Gamma(\alpha_2)}  x_{2}^{(\alpha_2 -1)} e^{-x_2} dX_2 \tag{5} 
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note also, that the density of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is given by:
&lt;span class=&#34;math display&#34; id=&#34;eq:Ydens&#34;&gt;\[\begin{align}
f_{Y}(y) &amp;amp;  = \frac{dP(Y \le y)}{dy}   \tag{6} 
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Applying the identity in &lt;a href=&#34;#eq:Ydens&#34;&gt;(6)&lt;/a&gt; to the integral in &lt;a href=&#34;#eq:YInt&#34;&gt;(5)&lt;/a&gt; and using the &lt;a href=&#34;https://en.wikipedia.org/wiki/Leibniz_integral_rule&#34;&gt;Leibniz integral rule&lt;/a&gt;, we get the following:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:YdensForm&#34;&gt;\[\begin{align}
f_{Y}(y) &amp;amp;  = \frac{d}{dy} \int_{0}^{\infty} \left( \int_{0}^{\frac{y}{1-y} X_2}  \frac{1}{\Gamma(\alpha_1)}  x_{1}^{(\alpha_1 -1)} e^{-x_1} dX_1 \right)  \frac{1}{\Gamma(\alpha_2)}  x_{2}^{(\alpha_2 -1)} e^{-x_2} dX_2  \nonumber \\
\implies f_{Y}(y) &amp;amp;  = \int_{0}^{\infty} \left( \frac{1}{\Gamma(\alpha_1)}  \left( \frac{y}{1-y} x_2 \right)^{(\alpha_1 -1)} \frac{e^{-(\frac{y}{1-y} x_2)}}{(1-y)^2} x_2 \right)  \frac{1}{\Gamma(\alpha_2)}  x_{2}^{(\alpha_2 -1)} e^{-x_2} dX_2   \nonumber \\
\implies f_{Y}(y) &amp;amp;  = \frac{y^{\alpha_1 - 1} \int_{0}^{\infty} \left( x_2^{\alpha_1-1} e^{-(\frac{y}{1-y} x_2)} x_2 \right) x_{2}^{(\alpha_2 -1)}  e^{-x_2} dX_2}{\Gamma(\alpha_1) \Gamma(\alpha_2) (1-y)^{\alpha_1 - 1}(1-y)^2} \nonumber \\
\implies f_{Y}(y) &amp;amp;  = \frac{y^{\alpha_1 - 1}}{\Gamma(\alpha_1) \Gamma(\alpha_2) (1-y)^{\alpha_1 + 1}} \int_{0}^{\infty} x_{2}^{(\alpha_1 + \alpha_2 -1)} e^{-(\frac{x_2}{1-y})} dX_2 \tag{7} 
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We now attempt to convert the integral in &lt;a href=&#34;#eq:YdensForm&#34;&gt;(7)&lt;/a&gt; into a form that is well known. We set &lt;span class=&#34;math inline&#34;&gt;\(Z = \frac{X_2}{1-y}\)&lt;/span&gt;, which gives us &lt;span class=&#34;math inline&#34;&gt;\(dX_2 = (1-y)dZ\)&lt;/span&gt;, and also note that &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; can take the range of values bounded by the interval &lt;span class=&#34;math inline&#34;&gt;\((0,\infty)\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(Z \rightarrow 0\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(X_2 \rightarrow 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Z \rightarrow \infty\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(X_2 \rightarrow \infty\)&lt;/span&gt;, since &lt;span class=&#34;math inline&#34;&gt;\(Y \rightarrow 0\)&lt;/span&gt;). Replacing &lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; in &lt;a href=&#34;#eq:YdensForm&#34;&gt;(7)&lt;/a&gt;, we get:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:YdensTrans&#34;&gt;\[\begin{align}
f_{Y}(y) &amp;amp;  = \frac{y^{\alpha_1 - 1}}{\Gamma(\alpha_1) \Gamma(\alpha_2) (1-y)^{\alpha_1 + 1}} \int_{0}^{\infty} ((1-y)z)^{(\alpha_1 + \alpha_2 -1)} e^{-z} (1-y) dz \nonumber \\
\implies f_{Y}(y) &amp;amp;  = \frac{y^{\alpha_1 - 1} (1-y)^{(\alpha_1 + \alpha_2)}}{\Gamma(\alpha_1) \Gamma(\alpha_2) (1-y)^{\alpha_1 + 1}} \int_{0}^{\infty} z^{(\alpha_1 + \alpha_2 -1)} e^{-z} dz \nonumber \\
\implies f_{Y}(y) &amp;amp;  = \frac{y^{\alpha_1 - 1} (1-y)^{\alpha_2 - 1}}{\Gamma(\alpha_1) \Gamma(\alpha_2)} \int_{0}^{\infty} z^{(\alpha_1 + \alpha_2 -1)} e^{-z} dz \tag{8} 
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The integral in &lt;a href=&#34;#eq:YdensTrans&#34;&gt;(8)&lt;/a&gt; is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Gamma_function&#34;&gt;Gamma function&lt;/a&gt;, whose value is given below:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:GammaForm&#34;&gt;\[\begin{align}
\int_{0}^{\infty} z^{(\alpha_1 + \alpha_2 -1)} e^{-z} dz = \Gamma(\alpha_1 + \alpha_2) \tag{9} 
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Substituting &lt;a href=&#34;#eq:GammaForm&#34;&gt;(9)&lt;/a&gt; back into &lt;a href=&#34;#eq:YdensTrans&#34;&gt;(8)&lt;/a&gt;, we get:
&lt;span class=&#34;math display&#34; id=&#34;eq:YdensFinal&#34;&gt;\[\begin{align}
f_{Y}(y) &amp;amp;  = \frac{y^{\alpha_1 - 1} (1-y)^{\alpha_2 - 1}}{\Gamma(\alpha_1) \Gamma(\alpha_2)} \Gamma(\alpha_1 + \alpha_2) \nonumber \\
\implies f_{Y}(y) &amp;amp;  = \frac{\Gamma(\alpha_1 + \alpha_2)}{\Gamma(\alpha_1) \Gamma(\alpha_2)} y^{\alpha_1 - 1} (1-y)^{\alpha_2 - 1}  \tag{10} \\
\implies f_{Y}(y) &amp;amp; = Beta(\alpha_1, \alpha_2) \nonumber
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;From &lt;a href=&#34;#eq:YdensFinal&#34;&gt;(10)&lt;/a&gt;, we have proved that the property in &lt;a href=&#34;#eq:BetaProp&#34;&gt;(1)&lt;/a&gt; holds for &lt;span class=&#34;math inline&#34;&gt;\(N=2\)&lt;/span&gt;. We now have to prove this for any general &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;. Now, let &lt;span class=&#34;math inline&#34;&gt;\(Z = \sum_{i=2}^{N} X_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y = \frac{X_1}{\sum_{i=1}^{N} X_i}\)&lt;/span&gt;, which gives us the following identities:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:Zdist&#34; id=&#34;eq:YexpZ&#34; id=&#34;eq:XnplSum&#34;&gt;\[\begin{align}
\sum_{i=1}^{N} X_i &amp;amp;  = X_{1} + Z \tag{11} \\
\implies Y  &amp;amp; = \frac{X_1}{\sum_{i=1}^{N} X_i} = \frac{X_1}{X_1 + Z} \tag{12} \\
Z &amp;amp; \sim Gamma(\sum_{i=2}^{N} \alpha_i,1) \tag{13}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where the result in &lt;a href=&#34;#eq:Zdist&#34;&gt;(13)&lt;/a&gt; was proved in the &lt;a href=&#34;https://jmodeler.github.io/post/additive-property-of-the-gamma-distribution/&#34;&gt;previous post&lt;/a&gt;. We see that the form of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; from &lt;a href=&#34;#eq:YexpZ&#34;&gt;(12)&lt;/a&gt; is similar to that of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; in the &lt;span class=&#34;math inline&#34;&gt;\(N=2\)&lt;/span&gt; case. Applying the result for &lt;span class=&#34;math inline&#34;&gt;\(N=2\)&lt;/span&gt; to &lt;a href=&#34;#eq:YexpZ&#34;&gt;(12)&lt;/a&gt;, we get:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:YZdensFinal&#34; id=&#34;eq:YZdist&#34;&gt;\[\begin{align}
Y &amp;amp; = \frac{X_1}{X_1 + Z} \sim Beta(\alpha_1, \sum_{i=2}^{N} \alpha_i)  \tag{14} \\
\implies  f_{Y}(y) &amp;amp; = \frac{\Gamma(\alpha_1 + \sum_{i=2}^{N} \alpha_i)}{\Gamma(\alpha_1) \Gamma(\sum_{i=2}^{N} \alpha_i)} y^{\alpha_1 - 1} (1-y)^{\sum_{i=2}^{N} \alpha_i - 1} \nonumber \\
\implies  f_{Y}(y) &amp;amp; = \frac{\Gamma(\sum_{i=1}^{N} \alpha_i)}{\Gamma(\alpha_1) \Gamma(\sum_{i=2}^{N} \alpha_i)} y^{\alpha_1 - 1} (1-y)^{\sum_{i=2}^{N} \alpha_i - 1} \tag{15}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Which is what we set out to prove.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Without loss of generality, I prove this property for &lt;span class=&#34;math inline&#34;&gt;\(i=1\)&lt;/span&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Additive Property of the Gamma Distribution</title>
      <link>/post/additive-property-of-the-gamma-distribution/</link>
      <pubDate>Tue, 03 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/additive-property-of-the-gamma-distribution/</guid>
      <description>


&lt;p&gt;While learning Bayesian Nonparametric methodology, I found a few properties useful in understanding the way Dirichlet Process Priors worked (this is a prior used very commonly in the field (&lt;span class=&#34;citation&#34;&gt;Ferguson (&lt;a href=&#34;#ref-ferguson1973bayesian&#34; role=&#34;doc-biblioref&#34;&gt;1973&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Antoniak (&lt;a href=&#34;#ref-antoniak1974mixtures&#34; role=&#34;doc-biblioref&#34;&gt;1974&lt;/a&gt;)&lt;/span&gt;)) . This post will be one in a series that describes each of the aforementioned properties and provides simple proofs/examples of the same.&lt;/p&gt;
&lt;p&gt;As the title suggests, we start with the Gamma distribution. We use the shape and rate parameterization of the distribution (as explained &lt;a href=&#34;https://en.wikipedia.org/wiki/Gamma_distribution&#34;&gt;here&lt;/a&gt;). Without loss of generality in this case, we also set the rate parameter $ = 1$ for all random variables unless otherwise mentioned. We want to prove the additive property of the gamma distribution:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:GammaSum&#34;&gt;\[\begin{align}
\sum_{i=1}^{N} X_i \sim Gamma(\sum_{i=1}^{N}\alpha_i, 1)                    \nonumber \\
 Where \, X_i \sim Gamma(\alpha_i, 1), \, \, i \in \{1,2, \dots, N\}  \tag{1} 
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where
&lt;span class=&#34;math display&#34; id=&#34;eq:GammaFun&#34;&gt;\[\begin{align}
Gamma(\alpha_i, 1) = f_{X_i}(x; \alpha_i) = \frac{1}{\Gamma(\alpha_i)}  x^{(\alpha_i -1)} e^{-x}                 \tag{2} 
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Importantly, the &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt;’s are independent. We prove the identity by induction. For &lt;span class=&#34;math inline&#34;&gt;\(N = 1\)&lt;/span&gt;, the property in &lt;a href=&#34;#eq:GammaSum&#34;&gt;(1)&lt;/a&gt; holds trivially. For &lt;span class=&#34;math inline&#34;&gt;\(N=2\)&lt;/span&gt;, we set about deriving this from first principles. Let &lt;span class=&#34;math inline&#34;&gt;\(Y = X_1 + X_2\)&lt;/span&gt;. We try to get the distribution of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. The following holds for &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:YdisInit&#34;&gt;\[\begin{align}
P(Y \le y)  &amp;amp; =  P(X_1 + X_2 \le y)   \nonumber \\
\implies P(Y \le y)  &amp;amp; = P(X_1 \le y - X_2)    \tag{3} 
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If we knew the value of &lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt;, the expression in &lt;a href=&#34;#eq:YdisInit&#34;&gt;(3)&lt;/a&gt; is obtained by integrating the probability density of &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt; over &lt;span class=&#34;math inline&#34;&gt;\((0, y-X_2)\)&lt;/span&gt;, given below:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:x1dis&#34;&gt;\[\begin{align}
P(X_1 \le y-X_2|X_2 = x_2)  &amp;amp; =  \int_{0}^{y-x_2}  \frac{1}{\Gamma(\alpha_1)}  x_{1}^{(\alpha_1 -1)} e^{-x_1} dX_1 = F(y, X_2; \alpha_1)   \tag{4} 
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;However, since &lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt; is a random variable varying from &lt;span class=&#34;math inline&#34;&gt;\((0, \infty)\)&lt;/span&gt;, the value of the expression in &lt;a href=&#34;#eq:YdisInit&#34;&gt;(3)&lt;/a&gt; is the expectation of &lt;span class=&#34;math inline&#34;&gt;\(F(y, X_2; \alpha_1)\)&lt;/span&gt; with respect to &lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt;, given below:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:ExpF&#34;&gt;\[\begin{align}
P(Y \le y) &amp;amp; = \int_{0}^{\infty} \left[ F(y, X_2;\alpha_1) \right]  \frac{1}{\Gamma(\alpha_2)}  x_{2}^{(\alpha_2 -1)} e^{-x_2} dX_2 \tag{5} 
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The expression in &lt;a href=&#34;#eq:ExpF&#34;&gt;(5)&lt;/a&gt; is a double integral, one to vary &lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt; over the range of values it can possibly take: &lt;span class=&#34;math inline&#34;&gt;\((0, \infty)\)&lt;/span&gt;, and the other to vary &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\((0, y-X_2)\)&lt;/span&gt;. Putting &lt;a href=&#34;#eq:x1dis&#34;&gt;(4)&lt;/a&gt; in &lt;a href=&#34;#eq:ExpF&#34;&gt;(5)&lt;/a&gt;, we get:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:YInt&#34;&gt;\[\begin{align}
P(Y \le y) &amp;amp;  = \int_{0}^{\infty} \left[ \int_{0}^{y-x_2}  \frac{1}{\Gamma(\alpha_1)}  x_{1}^{(\alpha_1 -1)} e^{-x_1} dX_1 \right]  \frac{1}{\Gamma(\alpha_2)}  x_{2}^{(\alpha_2 -1)} e^{-x_2} dX_2 \tag{6} 
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note also, that the density of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is given by:
&lt;span class=&#34;math display&#34; id=&#34;eq:Ydens&#34;&gt;\[\begin{align}
f_{Y}(y) &amp;amp;  = \frac{dP(Y \le y)}{dy}   \tag{7} 
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Applying the identity in &lt;a href=&#34;#eq:Ydens&#34;&gt;(7)&lt;/a&gt; to the integral in &lt;a href=&#34;#eq:YInt&#34;&gt;(6)&lt;/a&gt; and using the &lt;a href=&#34;https://en.wikipedia.org/wiki/Leibniz_integral_rule&#34;&gt;Leibniz integral rule&lt;/a&gt;, we get the following:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:YdensForm&#34;&gt;\[\begin{align}
f_{Y}(y) &amp;amp;  = \frac{d}{dy} \int_{0}^{\infty} \left( \int_{0}^{y-x_2}  \frac{1}{\Gamma(\alpha_1)}  x_{1}^{(\alpha_1 -1)} e^{-x_1} dX_1 \right)  \frac{1}{\Gamma(\alpha_2)}  x_{2}^{(\alpha_2 -1)} e^{-x_2} dX_2 \nonumber \\
\implies f_{Y}(y) &amp;amp;  = \int_{0}^{\infty} \left( \frac{1}{\Gamma(\alpha_1)}  (y-x_2)^{(\alpha_1 -1)} e^{-(y-x_2)} \right)  \frac{1}{\Gamma(\alpha_2)}  x_{2}^{(\alpha_2 -1)} e^{-x_2} dX_2 \nonumber \\
\implies f_{Y}(y) &amp;amp;  = \int_{0}^{\infty} \frac{1}{\Gamma(\alpha_1)} \frac{1}{\Gamma(\alpha_2)} (y-x_2)^{(\alpha_1 -1)} x_{2}^{(\alpha_2 -1)} e^{-y} dX_2 \tag{8} 
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We now attempt to convert the integral in &lt;a href=&#34;#eq:YdensForm&#34;&gt;(8)&lt;/a&gt; into a form that is well known. We set &lt;span class=&#34;math inline&#34;&gt;\(Z = \frac{X_2}{y}\)&lt;/span&gt;, which gives us &lt;span class=&#34;math inline&#34;&gt;\(dX_2 = ydZ\)&lt;/span&gt;, and also note that &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; can take the range of values bounded by the interval &lt;span class=&#34;math inline&#34;&gt;\((0,1)\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(Z \rightarrow 0\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(X_2 \rightarrow 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Z \rightarrow 1\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(X_2 \rightarrow \infty\)&lt;/span&gt;, since &lt;span class=&#34;math inline&#34;&gt;\(Y \rightarrow \infty\)&lt;/span&gt; too). Replacing &lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; in &lt;a href=&#34;#eq:YdensForm&#34;&gt;(8)&lt;/a&gt;, we get:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:YdensTrans&#34;&gt;\[\begin{align}
f_{Y}(y) &amp;amp;  = \int_{0}^{1} \frac{1}{\Gamma(\alpha_1)} \frac{1}{\Gamma(\alpha_2)} (y- yz)^{(\alpha_1 -1)} (yz)^{(\alpha_2 -1)} e^{-y} ydZ \nonumber \\
\implies f_{Y}(y) &amp;amp;  = \int_{0}^{1} \frac{1}{\Gamma(\alpha_1)} \frac{1}{\Gamma(\alpha_2)} y^{(\alpha_1 -1)} (1- z)^{(\alpha_1 -1)} y^{(\alpha_2 -1)} z^{(\alpha_2 -1)} e^{-y} ydZ \nonumber \\
\implies f_{Y}(y) &amp;amp;  = \int_{0}^{1} \frac{1}{\Gamma(\alpha_1)} \frac{1}{\Gamma(\alpha_2)} y^{(\alpha_1 + \alpha_2 -1)} e^{-y} (1- z)^{(\alpha_1 -1)} z^{(\alpha_2 -1)}  dZ \nonumber \\
\implies f_{Y}(y) &amp;amp;  =  \frac{1}{\Gamma(\alpha_1)} \frac{1}{\Gamma(\alpha_2)} y^{(\alpha_1 + \alpha_2 -1)} e^{-y} \int_{0}^{1} (1- z)^{(\alpha_1 -1)} z^{(\alpha_2 -1)}  dZ \tag{9} 
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The integral in &lt;a href=&#34;#eq:YdensTrans&#34;&gt;(9)&lt;/a&gt; is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Beta_function&#34;&gt;Beta function&lt;/a&gt;, whose value is given below:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:BetaForm&#34;&gt;\[\begin{align}
\int_{0}^{1} (1- z)^{(\alpha_1 -1)} z^{(\alpha_2 -1)}  dZ = \frac{\Gamma(\alpha_1)\Gamma(\alpha_2)}{\Gamma(\alpha_1 + \alpha_2)} \tag{10}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Substituting &lt;a href=&#34;#eq:BetaForm&#34;&gt;(10)&lt;/a&gt; back into &lt;a href=&#34;#eq:YdensTrans&#34;&gt;(9)&lt;/a&gt;, we get:
&lt;span class=&#34;math display&#34; id=&#34;eq:YdensFinal&#34;&gt;\[\begin{align}
f_{Y}(y) &amp;amp;  =  \frac{1}{\Gamma(\alpha_1)} \frac{1}{\Gamma(\alpha_2)} y^{(\alpha_1 + \alpha_2 -1)} e^{-y}  \frac{\Gamma(\alpha_1)\Gamma(\alpha_2)}{\Gamma(\alpha_1 + \alpha_2)}\nonumber \\
\implies f_{Y}(y) &amp;amp;  =  \frac{1}{\Gamma(\alpha_1 + \alpha_2)} y^{(\alpha_1 + \alpha_2 -1)} e^{-y}  \tag{11} \\
\implies f_{Y}(y) &amp;amp; = Gamma(\alpha_1 + \alpha_2, 1) \nonumber
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;From &lt;a href=&#34;#eq:YdensFinal&#34;&gt;(11)&lt;/a&gt;, we have now proved that the property in &lt;a href=&#34;#eq:GammaSum&#34;&gt;(1)&lt;/a&gt; holds for &lt;span class=&#34;math inline&#34;&gt;\(N=2\)&lt;/span&gt;. We now assume this is true for &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; and show that this property holds for &lt;span class=&#34;math inline&#34;&gt;\(N+1\)&lt;/span&gt;. Now, let &lt;span class=&#34;math inline&#34;&gt;\(Y = \sum_{i=1}^{N} X_i\)&lt;/span&gt;, which gives us the following identities:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:YNdist&#34; id=&#34;eq:XnplSum&#34;&gt;\[\begin{align}
\sum_{i=1}^{N+1} X_i &amp;amp;  = Y + X_{N+1} \tag{12} \\
Y &amp;amp; \sim Gamma(\sum_{i=1}^{N} \alpha_i,1) \tag{13}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;#eq:YNdist&#34;&gt;(13)&lt;/a&gt; follows from the induction assumption. Since the property in &lt;a href=&#34;#eq:GammaSum&#34;&gt;(1)&lt;/a&gt; is true for &lt;span class=&#34;math inline&#34;&gt;\(N=2\)&lt;/span&gt;, we combine this with the identities in &lt;a href=&#34;#eq:XnplSum&#34;&gt;(12)&lt;/a&gt; &amp;amp; &lt;a href=&#34;#eq:YNdist&#34;&gt;(13)&lt;/a&gt; to get:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:YNpldist&#34;&gt;\[\begin{align}
\sum_{i=1}^{N+1} X_i &amp;amp;  = Y + X_{N+1} \sim Gamma(\sum_{i=1}^{N} \alpha_i + \alpha_{N+1},1) \nonumber \\
\implies \sum_{i=1}^{N+1} X_i &amp;amp; \sim Gamma(\sum_{i=1}^{N+1} \alpha_i,1) \tag{14} 
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;#eq:YNpldist&#34;&gt;(14)&lt;/a&gt; shows that the property in &lt;a href=&#34;#eq:GammaSum&#34;&gt;(1)&lt;/a&gt; is true for &lt;span class=&#34;math inline&#34;&gt;\(N+1\)&lt;/span&gt;, completing the proof.&lt;/p&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-antoniak1974mixtures&#34;&gt;
&lt;p&gt;Antoniak, Charles E. 1974. “Mixtures of Dirichlet Processes with Applications to Bayesian Nonparametric Problems.” &lt;em&gt;The Annals of Statistics&lt;/em&gt;, 1152–74.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-ferguson1973bayesian&#34;&gt;
&lt;p&gt;Ferguson, Thomas S. 1973. “A Bayesian Analysis of Some Nonparametric Problems.” &lt;em&gt;The Annals of Statistics&lt;/em&gt;, 209–30.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Numerical Integration With Sparse Grids</title>
      <link>/post/numerical-integration-with-sparse-grids/</link>
      <pubDate>Sun, 28 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/numerical-integration-with-sparse-grids/</guid>
      <description>


&lt;p&gt;I recently read a paper &lt;span class=&#34;citation&#34;&gt;(Heiss and Winschel &lt;a href=&#34;#ref-heiss2008likelihood&#34;&gt;2008&lt;/a&gt;)&lt;/span&gt; that advocated the use of certain techniques (Sparse Grids, SG henceforth) in numerical integration to calculate likelihood functions, as opposed to using Monte Carlo (MC henceforth) methods for the same. While approximating integrals with MC methods are simpler to implement, they might lead to integral values with considerable simulation error &lt;span class=&#34;citation&#34;&gt;(Skrainka and Judd &lt;a href=&#34;#ref-skrainka2011high&#34;&gt;2011&lt;/a&gt;)&lt;/span&gt;. This post attempts to demonstrate the claim in &lt;span class=&#34;citation&#34;&gt;Skrainka and Judd (&lt;a href=&#34;#ref-skrainka2011high&#34;&gt;2011&lt;/a&gt;)&lt;/span&gt; using two very simple integrals, to which we already know the value. I attempt to compare the outcomes from using MC and SG.&lt;/p&gt;
&lt;p&gt;The integrals I’ll be evaluating are:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:sumint&#34;&gt;\[\begin{equation}
\int_{-\infty}^{\infty} \left( \sum_{i=1}^{5} x_i \right) dF_{X}   \tag{1} 
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and
&lt;span class=&#34;math display&#34; id=&#34;eq:prodint&#34;&gt;\[\begin{equation}
\int_{-\infty}^{\infty} \left( \prod_{i=1}^{5} x_i^2 \right) dF_{X} \tag{2}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(X = \{x_i\}_{i=1}^{5}\)&lt;/span&gt; is a five dimensional random variable, which is distributed according to the multivariate standard normal:
&lt;span class=&#34;math display&#34;&gt;\[
X \sim N\left( \left[ \begin{array}
{r}
0  \\
0  \\
0  \\
0  \\
0  \\
\end{array}\right], \left[ \begin{array}
{rrrrr}
1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0   \\
0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0  \\
0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0  \\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0  \\
0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1  \\
\end{array}\right] \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Given the distribution of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, the values of the integrals above are easily obtained from standard results (the value of &lt;a href=&#34;#eq:sumint&#34;&gt;(1)&lt;/a&gt; is &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; and that of &lt;a href=&#34;#eq:prodint&#34;&gt;(2)&lt;/a&gt; is &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;) respectively.&lt;/p&gt;
&lt;p&gt;I write some utility functions in R to compute the integrands above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#function to compute the sum of components of the random vector
s &amp;lt;- function(x)
{
  return(sum(x))
}

#function to compute the square product of the components of the random vector
p &amp;lt;- function(x)
{
  return(prod(x^2))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I now write a function that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Simulates a certain number of draws from the distribution of the random variable &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Computes the integrand function using each of these draws as input&lt;/li&gt;
&lt;li&gt;Takes the average of the values computed in the previous step&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This function, in effect, would give us the approximate value of the integral via MC methodology.&lt;/p&gt;
&lt;p&gt;The code is provided below, note that I use the &lt;tt&gt;&lt;a href=&#34;https://cran.r-project.org/web/packages/mvtnorm/index.html&#34;&gt;mvtnorm&lt;/a&gt;&lt;/tt&gt; package to create random draws.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(mvtnorm)

#Function to calculate the MC approximation for the integral
mc_int &amp;lt;- function(s, n, mu, sigma)
{
  #generate random draws
  x &amp;lt;- rmvnorm(n, mean = mu, sigma = sigma)
  #now get the integral
  mc_int_n &amp;lt;- mean(apply(x, 1, s))
  return(mc_int_n)
}

set.seed(100)
n &amp;lt;- 1000
mc_val &amp;lt;- mc_int(s, n, mu = rep(0,5), sigma = diag(5))
mc_val&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.007150433&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The result, 0.00715 is not far off from the true value of &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; at first glance, however, we need to compare this to the result from the SG approach.&lt;/p&gt;
&lt;p&gt;R has a package that generates sparse grids for numerical integration as described in &lt;span class=&#34;citation&#34;&gt;Heiss and Winschel (&lt;a href=&#34;#ref-heiss2008likelihood&#34;&gt;2008&lt;/a&gt;)&lt;/span&gt;, called &lt;tt&gt;&lt;a href=&#34;https://cran.r-project.org/web/packages/SparseGrid/index.html&#34;&gt;SparseGrid&lt;/a&gt;&lt;/tt&gt;. We now use the nodes and weights generated from this package to approximate the first integral.
I re-use some of the code provided in the documentation for the &lt;tt&gt;&lt;a href=&#34;https://cran.r-project.org/web/packages/SparseGrid/vignettes/SparseGrid.pdf&#34;&gt;SparseGrid&lt;/a&gt;&lt;/tt&gt; package in R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(SparseGrid)

#generate sparse grids for a 5 dimensional RV with accuracy level 2
sg &amp;lt;- createSparseGrid(type=&amp;#39;KPN&amp;#39;, dimension=5, k=2)


sg_int &amp;lt;- function(func, sg, ...)
{
  gx &amp;lt;- apply(sg$nodes, 1, function(x) {func(x, ...)})
  return(sum(gx * sg$weights))
}

sg_val &amp;lt;- sg_int(s, sg)
sg_val&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The result here is exactly 0. In light of this finding, the value obtained from the MC approach, in comparison, is a little off, and tends to show a high variance in output:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(100)

mc_int(s, n, mu = rep(0,5), sigma = diag(5))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.007150433&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mc_int(s, n, mu = rep(0,5), sigma = diag(5))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.03162326&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mc_int(s, n, mu = rep(0,5), sigma = diag(5))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.1287932&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mc_int(s, n, mu = rep(0,5), sigma = diag(5))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.01040134&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mc_int(s, n, mu = rep(0,5), sigma = diag(5))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.03798655&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the third case, there is a &lt;span class=&#34;math inline&#34;&gt;\(-12\%\)&lt;/span&gt; error(!) in the value of the computed integral when compared to the result from the SG approach. The SG approach, in addition, shows no such variation in repeated runs, since the grid values and weights are fixed for a given accuracy level and dimension (of the variable being integrated).&lt;/p&gt;
&lt;p&gt;I repeat the calculations for the second integral, as shown below&lt;/p&gt;
&lt;p&gt;MC approach:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(100)
n &amp;lt;- 1000
mc_val &amp;lt;- mc_int(p, n, mu = rep(0,5), sigma = diag(5))
mc_val&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.001089&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;SG approach:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#generate sparse grids for a 5 dimensional RV with accuracy level 6
sg &amp;lt;- createSparseGrid(type=&amp;#39;KPN&amp;#39;, dimension=5, k=6)
sg_val &amp;lt;- sg_int(p, sg)
sg_val&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once again, the SG approach gives us an exact value (note that the value of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, the accuracy level, has gone up, since the integrand is a higher order function). Again, the difference of the results between the two approaches doesn’t seem that large. However, variability of the results from the MC approach is still a concern, as shown below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(100)

mc_int(p, n, mu = rep(0,5), sigma = diag(5))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.001089&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mc_int(p, n, mu = rep(0,5), sigma = diag(5))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.4672555&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mc_int(p, n, mu = rep(0,5), sigma = diag(5))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.14692&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mc_int(p, n, mu = rep(0,5), sigma = diag(5))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.062975&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mc_int(p, n, mu = rep(0,5), sigma = diag(5))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9112416&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the second case, there is a roughly &lt;span class=&#34;math inline&#34;&gt;\(53\%\)&lt;/span&gt; (!!) error when compared to the true value of the integral. This variability could be worse with more complicated integrands.&lt;/p&gt;
&lt;p&gt;One suggestion to reduce variability in MC methods is to increase the number of draws, but that would entail a lot of calculations and result in longer runtimes.&lt;/p&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-mvt&#34;&gt;
&lt;p&gt;Genz, Alan, Frank Bretz, Tetsuhisa Miwa, Xuefei Mi, Friedrich Leisch, Fabian Scheipl, and Torsten Hothorn. 2019. &lt;em&gt;mvtnorm: Multivariate Normal and T Distributions&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=mvtnorm&#34;&gt;https://CRAN.R-project.org/package=mvtnorm&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-heiss2008likelihood&#34;&gt;
&lt;p&gt;Heiss, Florian, and Viktor Winschel. 2008. “Likelihood Approximation by Numerical Integration on Sparse Grids.” &lt;em&gt;Journal of Econometrics&lt;/em&gt; 144 (1): 62–80.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-skrainka2011high&#34;&gt;
&lt;p&gt;Skrainka, Benjamin S, and Kenneth L Judd. 2011. “High Performance Quadrature Rules: How Numerical Integration Affects a Popular Model of Product Differentiation.” &lt;em&gt;Available at SSRN 1870703&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
